<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Bayesian Method with TensorFlow Chapter 1. Introduction - 2. 베이지안 기초공사 - Oh Data Science</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Bayesian Method with TensorFlow Chapter 1. Introduction - 2. 베이지안 기초공사" />
<meta property="og:description" content="Bayesian Method with TensorFlow - Chapter1 Introduction 2. 베이지안 기초공사 우리는 베이지안 처럼 생각할 때 확률로 해석될 수 있는 &ldquo;믿음&quot;에 관심이 있습니다. 우리는 A라는 사건에 대해 믿음을 가지고 있고, 그 믿음은 과거의 정보에 의해서 만들어진 것입니다. 예를 들자면 이전의 테스트에 의해 우리의 코드가 버그가 있는지 사전 믿음을 주는 것이죠.
두 번째로, 우리는 증거를 찾습니다. 버그가 있는 코드 예시로 계속하자면, 만일 우리의 코드가 $X$개의 테스트들을 통과했다면, 우리는 우리의 믿음을 이것과 결합시키길 원할것입니다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/framework/" />
<meta property="article:published_time" content="2020-08-28T02:25:06+09:00" />
<meta property="article:modified_time" content="2020-08-28T02:25:06+09:00" />

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="Oh Data Science" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">Oh Data Science</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Bayesian Method with TensorFlow Chapter 1. Introduction - 2. 베이지안 기초공사</h1>
			
		</header><div class="content post__content clearfix">
			<h1 id="bayesian-method-with-tensorflow---chapter1-introduction"><strong>Bayesian Method with TensorFlow - Chapter1 Introduction</strong></h1>
<h1 id="2-베이지안-기초공사">2. 베이지안 기초공사</h1>
<p>우리는 베이지안 처럼 생각할 때 확률로 해석될 수 있는 &ldquo;믿음&quot;에 관심이 있습니다. 우리는 A라는 사건에 대해 믿음을 가지고 있고, 그 믿음은 과거의 정보에 의해서 만들어진 것입니다. 예를 들자면 이전의 테스트에 의해 우리의 코드가 버그가 있는지 사전 믿음을 주는 것이죠.</p>
<p>두 번째로, 우리는 증거를 찾습니다. 버그가 있는 코드 예시로 계속하자면, 만일 우리의 코드가 $X$개의 테스트들을 통과했다면, 우리는 우리의 믿음을 이것과 결합시키길 원할것입니다. 우리는 이 새로운 믿음을 &ldquo;사후 확률(Posterior Probability&quot;라고 부릅니다. 우리의 믿음을 업데이트 해나가는 것은 &ldquo;베이즈 정리&quot;라고 불리는 다음의 방정식을 이용해 진행됩니다.</p>
<p>$$ P(A|X) = \frac{P(X | A) P(A) }{P(X) } $$</p>
<p>$$ P(A|X) \propto{P(X | A) P(A) } $$</p>
<p>NOTE: ($\propto$ 는 &ldquo;비례한다&quot;라는 뜻입니다)</p>
<p>위의 식은 베이지안 추론에만 쓰이는 것은 아니라 다른 곳에서도 쓰이는 수학적인 팩트입니다. 베이지안 추론에서는 보통 이것을 사전 확률 $P(A)$와 사후 확률 $P(A|X)$를 연결하기 위해 사용합니다.</p>
<h3 id="코딩을-위한-사전-설정">코딩을 위한 사전 설정</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#@title Imports and Global Variables (make sure to run this cell)  { display-mode: &#34;form&#34; }</span>

<span style="color:#66d9ef">try</span>:
  <span style="color:#75715e"># %tensorflow_version only exists in Colab.</span>
  <span style="color:#f92672">%</span>tensorflow_version <span style="color:#ae81ff">2.</span>x
<span style="color:#66d9ef">except</span> <span style="color:#a6e22e">Exception</span>:
  <span style="color:#66d9ef">pass</span>


<span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> absolute_import, division, print_function


<span style="color:#75715e">#@markdown This sets the warning status (default is `ignore`, since this notebook runs correctly)</span>
warning_status <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;ignore&#34;</span> <span style="color:#75715e">#@param [&#34;ignore&#34;, &#34;always&#34;, &#34;module&#34;, &#34;once&#34;, &#34;default&#34;, &#34;error&#34;]</span>
<span style="color:#f92672">import</span> warnings
warnings<span style="color:#f92672">.</span>filterwarnings(warning_status)
<span style="color:#66d9ef">with</span> warnings<span style="color:#f92672">.</span>catch_warnings():
    warnings<span style="color:#f92672">.</span>filterwarnings(warning_status, category<span style="color:#f92672">=</span><span style="color:#a6e22e">DeprecationWarning</span>)
    warnings<span style="color:#f92672">.</span>filterwarnings(warning_status, category<span style="color:#f92672">=</span><span style="color:#a6e22e">UserWarning</span>)

<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> os
<span style="color:#75715e">#@markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/)</span>
matplotlib_style <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;fivethirtyeight&#39;</span> <span style="color:#75715e">#@param [&#39;fivethirtyeight&#39;, &#39;bmh&#39;, &#39;ggplot&#39;, &#39;seaborn&#39;, &#39;default&#39;, &#39;Solarize_Light2&#39;, &#39;classic&#39;, &#39;dark_background&#39;, &#39;seaborn-colorblind&#39;, &#39;seaborn-notebook&#39;]</span>
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt; plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(matplotlib_style)
<span style="color:#f92672">import</span> matplotlib.axes <span style="color:#f92672">as</span> axes;
<span style="color:#f92672">from</span> matplotlib.patches <span style="color:#f92672">import</span> Ellipse
<span style="color:#75715e">#%matplotlib inline</span>
<span style="color:#f92672">import</span> seaborn <span style="color:#f92672">as</span> sns; sns<span style="color:#f92672">.</span>set_context(<span style="color:#e6db74">&#39;notebook&#39;</span>)
<span style="color:#f92672">from</span> IPython.core.pylabtools <span style="color:#f92672">import</span> figsize
<span style="color:#75715e">#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)</span>
notebook_screen_res <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;retina&#39;</span> <span style="color:#75715e">#@param [&#39;retina&#39;, &#39;png&#39;, &#39;jpeg&#39;, &#39;svg&#39;, &#39;pdf&#39;]</span>
<span style="color:#75715e">#%config InlineBackend.figure_format = notebook_screen_res</span>

<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf

<span style="color:#f92672">import</span> tensorflow_probability <span style="color:#f92672">as</span> tfp
tfd <span style="color:#f92672">=</span> tfp<span style="color:#f92672">.</span>distributions
tfb <span style="color:#f92672">=</span> tfp<span style="color:#f92672">.</span>bijectors

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">_TFColor</span>(object):
    <span style="color:#e6db74">&#34;&#34;&#34;Enum of colors used in TF docs.&#34;&#34;&#34;</span>
    red <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#F15854&#39;</span>
    blue <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#5DA5DA&#39;</span>
    orange <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#FAA43A&#39;</span>
    green <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#60BD68&#39;</span>
    pink <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#F17CB0&#39;</span>
    brown <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#B2912F&#39;</span>
    purple <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#B276B2&#39;</span>
    yellow <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#DECF3F&#39;</span>
    gray <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#4D4D4D&#39;</span>
    <span style="color:#66d9ef">def</span> __getitem__(self, i):
        <span style="color:#66d9ef">return</span> [
            self<span style="color:#f92672">.</span>red,
            self<span style="color:#f92672">.</span>orange,
            self<span style="color:#f92672">.</span>green,
            self<span style="color:#f92672">.</span>blue,
            self<span style="color:#f92672">.</span>pink,
            self<span style="color:#f92672">.</span>brown,
            self<span style="color:#f92672">.</span>purple,
            self<span style="color:#f92672">.</span>yellow,
            self<span style="color:#f92672">.</span>gray,
        ][i <span style="color:#f92672">%</span> <span style="color:#ae81ff">9</span>]
TFColor <span style="color:#f92672">=</span> _TFColor()

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">session_options</span>(enable_gpu_ram_resizing<span style="color:#f92672">=</span>True, enable_xla<span style="color:#f92672">=</span>False):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Allowing the notebook to make use of GPUs if they&#39;re available.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear
</span><span style="color:#e6db74">    algebra that optimizes TensorFlow computations.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    config <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>config
    gpu_devices <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>list_physical_devices(<span style="color:#e6db74">&#39;GPU&#39;</span>)
    <span style="color:#66d9ef">if</span> enable_gpu_ram_resizing:
        <span style="color:#66d9ef">for</span> device <span style="color:#f92672">in</span> gpu_devices:
           tf<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>set_memory_growth(device, True)
    <span style="color:#66d9ef">if</span> enable_xla:
        config<span style="color:#f92672">.</span>optimizer<span style="color:#f92672">.</span>set_jit(True)
    <span style="color:#66d9ef">return</span> config

session_options(enable_gpu_ram_resizing<span style="color:#f92672">=</span>True, enable_xla<span style="color:#f92672">=</span>True)
</code></pre></div><pre><code>&lt;module 'tensorflow._api.v2.config' from '/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/config/__init__.py'&gt;
</code></pre>
<h1 id="예제--동전-던지기"><strong>예제 : 동전 던지기</strong></h1>
<p>모든 통계학 서적은 동전던지기 문제를 포함하고 있으니까 우선 이걸 처리하도록 합시다. 당신이 바보라서 동전 던지기에서 앞면이 나올 확률을 모른다고 가정합시다( 스포주의 : 정답은 50%입니다). 당신은 앞면이 나오는 정확한 비율이 있다고 믿고 그것을 $p$라고 부릅시다. 그러나 $p$가 무엇인지에 대한 사전 지식은 없습니다</p>
<p>자 동전을 던져봅시다. 그리고 결과를 H(앞면), T(뒷면)으로 기록합시다. 이것이 우리의 관찰된 데이터입니다. 어떻게 우리가 더 많은 데이터를 발견할 수록 우리의 추론을 바꿔나가야 할까요? 특히 우리가 작은 데이터를 가지고 있을 때의 사후 확률과 많은 데이터를 가지고 있을 때의 사후 확률은 어떻게 다를까요?</p>
<p>밑에서 우리는 점점 데이터가 많아질 수록(동전을 많이 던질 수록) 사후 확률이 업데이트되어지는 과정을 그래프로 그려볼겁니다. 또한 이것은 텐서에 값을 넣고 데이터로 그래프를 그리는 훌륭한 예제가 될 것입니다!</p>
<p>처음으로 우리의 Tensorflow 그래프의 값들을 정의해봅시다</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># 그래프 만들기</span>
rv_coin_flip_prior <span style="color:#f92672">=</span> tfp<span style="color:#f92672">.</span>distributions<span style="color:#f92672">.</span>Bernoulli(probs<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>int32)
<span style="color:#75715e"># 사전 확률이 $p$가 0.5인 베르누이 분포를 따른다고 가정합시다</span>
num_trials <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant([<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">15</span>, <span style="color:#ae81ff">50</span>, <span style="color:#ae81ff">500</span>, <span style="color:#ae81ff">1000</span>, <span style="color:#ae81ff">2000</span>])

coin_flip_data <span style="color:#f92672">=</span> rv_coin_flip_prior<span style="color:#f92672">.</span>sample(num_trials[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]) <span style="color:#75715e"># 2000번 던져보아요</span>

<span style="color:#75715e"># 0번 던진건 0으로 놓기 위해서 앞에 0을 넣읍시다</span>
coin_flip_data <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>pad(coin_flip_data,tf<span style="color:#f92672">.</span>constant([[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>,]]),<span style="color:#e6db74">&#34;CONSTANT&#34;</span>)

<span style="color:#75715e"># 0번~2000번 까지 중에 앞면이 몇 번 나왔는지 앞의 num_trials 숫자 간격대로 세봅시다</span>
cumulative_headcounts <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>gather(tf<span style="color:#f92672">.</span>cumsum(coin_flip_data), num_trials)

rv_observed_heads <span style="color:#f92672">=</span> tfp<span style="color:#f92672">.</span>distributions<span style="color:#f92672">.</span>Beta(
    concentration1<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>cast(<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> cumulative_headcounts, tf<span style="color:#f92672">.</span>float32),
    concentration0<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>cast(<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> num_trials <span style="color:#f92672">-</span> cumulative_headcounts, tf<span style="color:#f92672">.</span>float32))
<span style="color:#75715e"># 앞면이 나온 횟수가 베타 분포를 따른다고 가정합시다(증거들이 베타 분포를 따른다고 가정하는것이죠)</span>

probs_of_heads <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>linspace(start<span style="color:#f92672">=</span><span style="color:#ae81ff">0.</span>, stop<span style="color:#f92672">=</span><span style="color:#ae81ff">1.</span>, num<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;linspace&#34;</span>)
observed_probs_heads <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>transpose(rv_observed_heads<span style="color:#f92672">.</span>prob(probs_of_heads[:, tf<span style="color:#f92672">.</span>newaxis]))
<span style="color:#75715e"># 각각에 확률을 배당합시다(R로 따지면 pbeta 함수로 만들기)</span>
</code></pre></div><p>자 이제 우리의 텐서들을 matplotlib으로 그려봅시다</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># For the already prepared, I&#39;m using Binomial&#39;s conj. prior.</span>
plt<span style="color:#f92672">.</span>figure(figsize(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">9</span>))
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(num_trials)):
    sx <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplot(len(num_trials)<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>, i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)
    plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;$p$, probability of heads&#34;</span>) \
    <span style="color:#66d9ef">if</span> i <span style="color:#f92672">in</span> [<span style="color:#ae81ff">0</span>, len(num_trials)<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#66d9ef">else</span> None
    plt<span style="color:#f92672">.</span>setp(sx<span style="color:#f92672">.</span>get_yticklabels(), visible<span style="color:#f92672">=</span>False)
    plt<span style="color:#f92672">.</span>plot(probs_of_heads, observed_probs_heads[i], 
             label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;observe </span><span style="color:#e6db74">%d</span><span style="color:#e6db74"> tosses,</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74"> </span><span style="color:#e6db74">%d</span><span style="color:#e6db74"> heads&#34;</span> <span style="color:#f92672">%</span> (num_trials[i], cumulative_headcounts[i]))
    plt<span style="color:#f92672">.</span>fill_between(probs_of_heads, <span style="color:#ae81ff">0</span>, observed_probs_heads[i], 
                     color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">3</span>], alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.4</span>)
    plt<span style="color:#f92672">.</span>vlines(<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">4</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;k&#34;</span>, linestyles<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;--&#34;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
    leg <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>legend()
    leg<span style="color:#f92672">.</span>get_frame()<span style="color:#f92672">.</span>set_alpha(<span style="color:#ae81ff">0.4</span>)
    plt<span style="color:#f92672">.</span>autoscale(tight<span style="color:#f92672">=</span>True)


plt<span style="color:#f92672">.</span>suptitle(<span style="color:#e6db74">&#34;Bayesian updating of posterior probabilities&#34;</span>, y<span style="color:#f92672">=</span><span style="color:#ae81ff">1.02</span>,
             fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">14</span>)
plt<span style="color:#f92672">.</span>tight_layout()
</code></pre></div><p><img src="https://user-images.githubusercontent.com/57588650/91476057-6c407e00-e8d7-11ea-846f-ed8ff0c7ae25.png" alt="output_14_0"></p>
<p>사후확률은 곡선으로 나타납니다. 그리고 우리의 불확실성은 곡선의 너비에 비례합니다. 위의 그래프에서 볼 수 있듯이, 우리의 사후확률들은 이리저리 움직이기 시작합니다. 마침내 우리가 점점 더 많은 데이터를 가져올 수록, 우리의 확률은 진짜 값인 $p=0.5$ 근처로 점점 더 타이트하게 모입니다.(점선으로 표현되어있습니다)</p>
<p>그래프들이 항상 0.5에서 가장 높은 값을 가지지 않는다는 것을 잘 봅시다. 그건 그렇게 되야할 이유가 하나도 없습니다. 우리가 $p$에 대해 아무런 사전 정보가 없다고 가정했다는 것을 기억해봅시다. 실제로, 우리가 8번 던져서 1번 앞면이 나오는 꽤 극단적인 케이스를 목격했다고 합시다. 그러면 우리의 분포는 0.5에서 꽤 치우쳐져 있는 것으로 보일 것입니다(아무런 사전 정보가 없다면, 당신이 8번중에 1번 앞면이 나온 동전이 멀쩡한 동전이라고 얼마나 자신있게 베팅할 수 있겠습니까?). 점점 더 많은 데이터가 쌓일 수록, 우리는 점점 더 많은 확률들이 $p = 0.5$에 할당된다는 것을 볼 수 있습니다. 물론 전부가 그렇지는 않죠.</p>
<p>다음 예시는 베이지안 추론의 수학적인 측면을 간단하게 나타냅니다</p>
<h2 id="예제--버그가-있을까요-없을까요">예제 : 버그가 있을까요 없을까요?</h2>
<p>$A$가 우리의 코드에 아무런 버그가 없는 사건이라고 합시다. 그리고 $X$는 우리의 코드가 모든 디버깅 테스트를 통과했다는 사건을 의미한다고 합시다. 이제부터 우리는 버그가 없다는 확률을 변수로 남겨놓을 것입니다. 예를 들면 $P(A) = p$라고 하죠.</p>
<p>우리는 $X$, 즉 모든 디버깅 테스트를 통과했을 때 버그가 없을 확률을 의미하는 $P(A|X)$에 관심있습니다. 위의 수식(베이즈 정리)을 활용하기 위해 우리는 몇 가지 계산해야할 값들이 있습니다.</p>
<p>$P(X|A)$는 무엇일까요? 바로 코드에 아무런 버그가 없을 때 모든 디버깅 테스트를 통과( = $X$)할 확률이겠죠? 음 이건 당연히 1일겁니다.</p>
<p>$P(X)$ 는 좀 더 어렵습니다. $X$라는 사건은 두 가지의 케이스로 나뉠 수 있죠. 바로 우리의 코드가 실제론 버그가 있음에도($A$가 아님을 의미하는 $~A$라고 씁시다) 모든 디버깅 테스트를 통과하는 경우와 실제로도 버그가 없고($A$) 모든 디버깅 테스트도 통과하는($X$) 경우입니다. 그럼 $P(X)$는 이렇게 표현될 수 있겠죠</p>
<p>$$
\begin{align*}
P(A|X) &amp;= \frac{P(X | A) P(A) }{P(X) }
\end{align*}
$$</p>
<p>$$
\begin{align*}
P(X) &amp;= P(X \text{ and } A) + P(X \text{ and } \sim A) \<br>
\end{align*}
$$</p>
<p>$$
\begin{align*}
&amp;= P(X|A)P(A) + P(X | \sim A)P(\sim A) \end{align*} $$
$$
\begin{align*}
&amp;= P(X|A)p + P(X | \sim A)(1-p) \end{align*} $$</p>
<p>우리는 이미 위에서 $P(X|A)$를 1로 계산했지만, $P(X|~A)$는 주관적인 영역입니다. 우리의 코드는 테스트들을 통과할 수 있지만 여전히 버그가 있을 수 있습니다. 그런데 버그가 존재할 확률은 줄어들겠죠. 이것이 테스트를 수행하는 횟수, 테스트들의 복잡도 등에 의존한다는 것을 생각해봅시다. 보수적으로 $P(X|~A)$ = 0.5라고 잡아봅시다. 그러면</p>
<p>$$ \begin{align*}
P(A | X) &amp;= \frac{1\cdot p}{ 1\cdot p +0.5 (1-p) } \<br>
&amp;= \frac{ 2 p}{1+p} \end{align*} $$</p>
<p>이렇게 계산됩니다. 이것이 바로 사후확률입니다. 그럼 이건 우리가 정한 사전 믿음인 $p$에 따라서 어떻게 변화할까요?</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># 확률은 0부터 1까지의 숫자입니다. 50개로 나눠보죠</span>
p <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>linspace(start<span style="color:#f92672">=</span><span style="color:#ae81ff">0.</span>, stop<span style="color:#f92672">=</span><span style="color:#ae81ff">1.</span>, num<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>)

<span style="color:#75715e"># 시각화 합시다</span>
plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12.5</span>, <span style="color:#ae81ff">6</span>))
plt<span style="color:#f92672">.</span>plot(p, <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>p<span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">+</span>p) , color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">3</span>], lw<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>) <span style="color:#75715e"># x축이 $p$이고 y축은 $\frac{ 2 p}{1+p}$인 그래프 입니다</span>
<span style="color:#75715e">#plt.fill_between(p, 2*p/(1+p), alpha=.5, facecolor=[&#34;#A60628&#34;])</span>
plt<span style="color:#f92672">.</span>scatter(<span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>(<span style="color:#ae81ff">0.2</span>)<span style="color:#f92672">/</span><span style="color:#ae81ff">1.2</span>, s<span style="color:#f92672">=</span><span style="color:#ae81ff">140</span>, c<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">3</span>])
plt<span style="color:#f92672">.</span>xlim(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)
plt<span style="color:#f92672">.</span>ylim(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;Prior, $P(A) = p$&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;Posterior, $P(A|X)$, with $P(A) = p$&#34;</span>)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;Are there bugs in my code?&#34;</span>);
</code></pre></div><p><img src="https://user-images.githubusercontent.com/57588650/91476235-a90c7500-e8d7-11ea-8bd9-f6dafde91381.png" alt="output_23_0"></p>
<p>여기서 볼 수 있는건, 사전 확률인 $p$가 낮을 때 디버깅 테스트를 모두 통과하면 서후 확률이 더 큰 폭으로 높아진다는 것입니다. 자 이제 사전 확률에 특정한 값을 넣어봅시다. 제가 생각할 땐 제가 뛰어난 프로그래머이기 때문에 현실적으로 제 코드에 20%의 확률로 버그가 없다고 가정하겠습니다. 더 현실적으로 하려면 이 코드가 얼마나 복잡한지에 대한 함수가 되어야겠지만 그냥 20%라고 해봅시다. 그러면 제 업데이트된 믿음, 즉 제 코드에 버그가 없을 것이란 사후 확률은 33%가 됩니다!(그래프의 점)</p>
<p>사전 믿음은 확률이란걸 기억합시다. $p$가 버그가 없을 확률이었고 당연히 $1-p$가 버그가 있을 확률일 것입니다.</p>
<p>비슷하게 우리의 사후 믿음도 확률로 나타내집니다.  $P(A|X)$의 확률로 모든 테스트를 통과하고 버그도 없을 것이고, $1-P(A|X)$의 확률로 모든 테스트를 통과했음에도 버그가 있을 것입니다. 우리의 사후 확률은 어떻게 생겼을까요? 밑의 그래프는 사전과 사후 확률을 나타냅니다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># 우리의 사전, 사후 확률을 정의합시다.</span>
prior <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant([<span style="color:#ae81ff">0.20</span>, <span style="color:#ae81ff">0.80</span>])
posterior <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant([<span style="color:#ae81ff">1.</span><span style="color:#f92672">/</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2.</span><span style="color:#f92672">/</span><span style="color:#ae81ff">3</span>])

<span style="color:#75715e"># 간단한 시각화</span>
plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12.5</span>, <span style="color:#ae81ff">4</span>))
colours <span style="color:#f92672">=</span> [TFColor[<span style="color:#ae81ff">0</span>], TFColor[<span style="color:#ae81ff">3</span>]]
plt<span style="color:#f92672">.</span>bar([<span style="color:#ae81ff">0</span>, <span style="color:#f92672">.</span><span style="color:#ae81ff">7</span>], prior, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.70</span>, width<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>,
        color<span style="color:#f92672">=</span>colours[<span style="color:#ae81ff">0</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;prior distribution&#34;</span>,
        lw<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;3&#34;</span>, edgecolor<span style="color:#f92672">=</span>colours[<span style="color:#ae81ff">0</span>])
plt<span style="color:#f92672">.</span>bar([<span style="color:#ae81ff">0</span><span style="color:#f92672">+</span><span style="color:#ae81ff">0.25</span>, <span style="color:#f92672">.</span><span style="color:#ae81ff">7</span><span style="color:#f92672">+</span><span style="color:#ae81ff">0.25</span>], posterior, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7</span>,
        width<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, color<span style="color:#f92672">=</span>colours[<span style="color:#ae81ff">1</span>],
        label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;posterior distribution&#34;</span>,
        lw<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;3&#34;</span>, edgecolor<span style="color:#f92672">=</span>colours[<span style="color:#ae81ff">1</span>])

plt<span style="color:#f92672">.</span>xticks([<span style="color:#ae81ff">0.20</span>, <span style="color:#f92672">.</span><span style="color:#ae81ff">95</span>], [<span style="color:#e6db74">&#34;Bugs Absent&#34;</span>, <span style="color:#e6db74">&#34;Bugs Present&#34;</span>])
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;Prior and Posterior probability of bugs present&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Probability&#34;</span>)
plt<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;upper left&#34;</span>);
</code></pre></div><p><img src="https://user-images.githubusercontent.com/57588650/91476261-b3c70a00-e8d7-11ea-9861-bbd7056b7a62.png" alt="output_26_0"></p>
<p>우리가 모든 테스트를 통과했다는걸 발견하면 버그가 없을 확률이 높아진다는 것을 잘 봅시다. 테스트의 갯수를 늘린다면 우리는 버그가 없다는 확신($p = 1$)에 다다를 수 있을 것입니다.</p>
<p>이것은 베이지안 추론과 베이즈 룰의 아주 간단한 예제입니다. 하지만 불행하게도 이렇게 인위적으로 만들어진 예시들 말고는 베이지안 추론을 하기 위해선 더 복잡한 수학적인 지식을 필요로 합니다. 처음으론 우리의 모델링 도구들을 더 확장해야하고 다음으론 확률 분포에 대해 배워야하죠. 만약 당신이 이미 그런 것을에 익숙하다면 넘어가져도 됩니다(훑어보셔도 돼요). 그런데 익숙하지 않다면, 다음 섹션은 필수적으로 보셔야 합니다!</p>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/bayesian/" rel="tag">Bayesian</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/tensorflow/" rel="tag">TensorFlow</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/python/" rel="tag">Python</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div class="authorbox clearfix">
	<div class="authorbox__header">
		<span class="authorbox__name">About Tai Hwan Oh</span>
	</div>
</div>



			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2021 Tai Hwan Oh.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>