<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Bayesian Method with TensorFlow Chapter 3. MCMC(Markov Chain Monte Carlo) - 3. MCMC 수렴성 진단과 팁들 - Oh Data Science</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Bayesian Method with TensorFlow Chapter 3. MCMC(Markov Chain Monte Carlo) - 3. MCMC 수렴성 진단과 팁들" />
<meta property="og:description" content="Bayesian Method with TensorFlow - Chapter3 MCMC(Markov Chain Monte Carlo) #@title Imports and Global Variables (make sure to run this cell) { display-mode: &#34;form&#34; } try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x except Exception: pass from __future__ import absolute_import, division, print_function #@markdown This sets the warning status (default is `ignore`, since this notebook runs correctly) warning_status = &#34;ignore&#34; #@param [&#34;ignore&#34;, &#34;always&#34;, &#34;module&#34;, &#34;once&#34;, &#34;default&#34;, &#34;error&#34;] import warnings warnings.filterwarnings(warning_status) with warnings." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/mcmclast/" />
<meta property="article:published_time" content="2020-09-12T16:12:38+09:00" />
<meta property="article:modified_time" content="2020-09-12T16:12:38+09:00" />

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="Oh Data Science" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">Oh Data Science</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Bayesian Method with TensorFlow Chapter 3. MCMC(Markov Chain Monte Carlo) - 3. MCMC 수렴성 진단과 팁들</h1>
			
		</header><div class="content post__content clearfix">
			<h1 id="bayesian-method-with-tensorflow---chapter3-mcmcmarkov-chain-monte-carlo"><strong>Bayesian Method with TensorFlow - Chapter3 MCMC(Markov Chain Monte Carlo)</strong></h1>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#@title Imports and Global Variables (make sure to run this cell)  { display-mode: &#34;form&#34; }</span>

<span style="color:#66d9ef">try</span>:
  <span style="color:#75715e"># %tensorflow_version only exists in Colab.</span>
  <span style="color:#f92672">%</span>tensorflow_version <span style="color:#ae81ff">2.</span>x
<span style="color:#66d9ef">except</span> <span style="color:#a6e22e">Exception</span>:
  <span style="color:#66d9ef">pass</span>


<span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> absolute_import, division, print_function


<span style="color:#75715e">#@markdown This sets the warning status (default is `ignore`, since this notebook runs correctly)</span>
warning_status <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;ignore&#34;</span> <span style="color:#75715e">#@param [&#34;ignore&#34;, &#34;always&#34;, &#34;module&#34;, &#34;once&#34;, &#34;default&#34;, &#34;error&#34;]</span>
<span style="color:#f92672">import</span> warnings
warnings<span style="color:#f92672">.</span>filterwarnings(warning_status)
<span style="color:#66d9ef">with</span> warnings<span style="color:#f92672">.</span>catch_warnings():
    warnings<span style="color:#f92672">.</span>filterwarnings(warning_status, category<span style="color:#f92672">=</span><span style="color:#a6e22e">DeprecationWarning</span>)
    warnings<span style="color:#f92672">.</span>filterwarnings(warning_status, category<span style="color:#f92672">=</span><span style="color:#a6e22e">UserWarning</span>)

<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> os
<span style="color:#75715e">#@markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/)</span>
matplotlib_style <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;fivethirtyeight&#39;</span> <span style="color:#75715e">#@param [&#39;fivethirtyeight&#39;, &#39;bmh&#39;, &#39;ggplot&#39;, &#39;seaborn&#39;, &#39;default&#39;, &#39;Solarize_Light2&#39;, &#39;classic&#39;, &#39;dark_background&#39;, &#39;seaborn-colorblind&#39;, &#39;seaborn-notebook&#39;]</span>
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt; plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(matplotlib_style)
<span style="color:#f92672">import</span> matplotlib.axes <span style="color:#f92672">as</span> axes;
<span style="color:#f92672">from</span> matplotlib.patches <span style="color:#f92672">import</span> Ellipse
<span style="color:#f92672">import</span> matplotlib <span style="color:#f92672">as</span> mpl
<span style="color:#75715e">#%matplotlib inline</span>
<span style="color:#f92672">import</span> seaborn <span style="color:#f92672">as</span> sns; sns<span style="color:#f92672">.</span>set_context(<span style="color:#e6db74">&#39;notebook&#39;</span>)
<span style="color:#f92672">from</span> IPython.core.pylabtools <span style="color:#f92672">import</span> figsize
<span style="color:#75715e">#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)</span>
notebook_screen_res <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;retina&#39;</span> <span style="color:#75715e">#@param [&#39;retina&#39;, &#39;png&#39;, &#39;jpeg&#39;, &#39;svg&#39;, &#39;pdf&#39;]</span>
<span style="color:#75715e">#%config InlineBackend.figure_format = notebook_screen_res</span>

<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf

<span style="color:#f92672">import</span> tensorflow_probability <span style="color:#f92672">as</span> tfp
tfd <span style="color:#f92672">=</span> tfp<span style="color:#f92672">.</span>distributions
tfb <span style="color:#f92672">=</span> tfp<span style="color:#f92672">.</span>bijectors

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">_TFColor</span>(object):
    <span style="color:#e6db74">&#34;&#34;&#34;Enum of colors used in TF docs.&#34;&#34;&#34;</span>
    red <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#F15854&#39;</span>
    blue <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#5DA5DA&#39;</span>
    orange <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#FAA43A&#39;</span>
    green <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#60BD68&#39;</span>
    pink <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#F17CB0&#39;</span>
    brown <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#B2912F&#39;</span>
    purple <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#B276B2&#39;</span>
    yellow <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#DECF3F&#39;</span>
    gray <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#4D4D4D&#39;</span>
    <span style="color:#66d9ef">def</span> __getitem__(self, i):
        <span style="color:#66d9ef">return</span> [
            self<span style="color:#f92672">.</span>red,
            self<span style="color:#f92672">.</span>orange,
            self<span style="color:#f92672">.</span>green,
            self<span style="color:#f92672">.</span>blue,
            self<span style="color:#f92672">.</span>pink,
            self<span style="color:#f92672">.</span>brown,
            self<span style="color:#f92672">.</span>purple,
            self<span style="color:#f92672">.</span>yellow,
            self<span style="color:#f92672">.</span>gray,
        ][i <span style="color:#f92672">%</span> <span style="color:#ae81ff">9</span>]
TFColor <span style="color:#f92672">=</span> _TFColor()

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">session_options</span>(enable_gpu_ram_resizing<span style="color:#f92672">=</span>True, enable_xla<span style="color:#f92672">=</span>False):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Allowing the notebook to make use of GPUs if they&#39;re available.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear
</span><span style="color:#e6db74">    algebra that optimizes TensorFlow computations.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    config <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>config
    gpu_devices <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>list_physical_devices(<span style="color:#e6db74">&#39;GPU&#39;</span>)
    <span style="color:#66d9ef">if</span> enable_gpu_ram_resizing:
        <span style="color:#66d9ef">for</span> device <span style="color:#f92672">in</span> gpu_devices:
           tf<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>set_memory_growth(device, True)
    <span style="color:#66d9ef">if</span> enable_xla:
        config<span style="color:#f92672">.</span>optimizer<span style="color:#f92672">.</span>set_jit(True)
    <span style="color:#66d9ef">return</span> config

session_options(enable_gpu_ram_resizing<span style="color:#f92672">=</span>True, enable_xla<span style="color:#f92672">=</span>True)

<span style="color:#960050;background-color:#1e0010">!</span>apt <span style="color:#f92672">-</span>qq <span style="color:#f92672">-</span>y install fonts<span style="color:#f92672">-</span>nanum
 
<span style="color:#f92672">import</span> matplotlib.font_manager <span style="color:#f92672">as</span> fm
fontpath <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf&#39;</span>
font <span style="color:#f92672">=</span> fm<span style="color:#f92672">.</span>FontProperties(fname<span style="color:#f92672">=</span>fontpath, size<span style="color:#f92672">=</span><span style="color:#ae81ff">9</span>)
plt<span style="color:#f92672">.</span>rc(<span style="color:#e6db74">&#39;font&#39;</span>, family<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;NanumBarunGothic&#39;</span>) 
mpl<span style="color:#f92672">.</span>font_manager<span style="color:#f92672">.</span>_rebuild()
</code></pre></div><pre><code>fonts-nanum is already the newest version (20170925-1).
The following package was automatically installed and is no longer required:
  libnvidia-common-440
Use 'apt autoremove' to remove it.
0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.
</code></pre>
<h1 id="3-수렴성-진단diagnosing-convergence"><strong>3. 수렴성 진단(Diagnosing Convergence)</strong></h1>
<h2 id="자기상관성autocorrelation"><strong>자기상관성(Autocorrelation)</strong></h2>
<p>자기상관성은 수열들 자신이 얼마나 상관관계가 있는지 측정하는 지표입니다. 1의 값은 완전한 양의 자기상관성을 의미하고, 0은 자기상관성이 없다는 것을, -1은 완전한 음의 자기상관성을 의미합니다. 만일 당신이 일반적인 *상관관계(correlation)*에 익숙하다면, 자기상관성은 단지 시간 $t$에서의 수열 $x_\tau$가 시간 $t-k$에서의 수열과 얼마나 상관관계가 있는지를 의미합니다.</p>
<p>$$R(k) = \text{Corr}( x_t, x_{t-k} ) $$</p>
<p>예를 들면 다음과 같은 두 개의 수열이 있다고 생각해봅시다.</p>
<p>$$x_t \sim \text{Normal}(0,1), x_0 = 0$$
$$y_t \sim \text{Normal}(y_{t-1}, 1 ), y_0 = 0$$</p>
<p>그리고 이 예시 수열들은 다음과 같은 경로를 가지고 있습니다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># evaluate 함수 생성</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">evaluate</span>(tensors):
    <span style="color:#66d9ef">if</span> tf<span style="color:#f92672">.</span>executing_eagerly():
         <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>nest<span style="color:#f92672">.</span>pack_sequence_as(
             tensors,
             [t<span style="color:#f92672">.</span>numpy() <span style="color:#66d9ef">if</span> tf<span style="color:#f92672">.</span>is_tensor(t) <span style="color:#66d9ef">else</span> t
             <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tf<span style="color:#f92672">.</span>nest<span style="color:#f92672">.</span>flatten(tensors)])
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>Session() <span style="color:#66d9ef">as</span> sess:
        <span style="color:#66d9ef">return</span> sess<span style="color:#f92672">.</span>run(tensors)

x_t <span style="color:#f92672">=</span> evaluate(tfd<span style="color:#f92672">.</span>Normal(loc<span style="color:#f92672">=</span><span style="color:#ae81ff">0.</span>, scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1.</span>)<span style="color:#f92672">.</span>sample(sample_shape<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>))
x_t[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
y_t <span style="color:#f92672">=</span> evaluate(tf<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">200</span>))
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">200</span>):
    y_t[i] <span style="color:#f92672">=</span> evaluate(tfd<span style="color:#f92672">.</span>Normal(loc<span style="color:#f92672">=</span>y_t[i <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>], scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1.</span>)<span style="color:#f92672">.</span>sample())

plt<span style="color:#f92672">.</span>figure(figsize(<span style="color:#ae81ff">12.5</span>, <span style="color:#ae81ff">4</span>))
plt<span style="color:#f92672">.</span>plot(y_t, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;$y_t$&#34;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
plt<span style="color:#f92672">.</span>plot(x_t, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;$x_t$&#34;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;time, $t$&#34;</span>)
plt<span style="color:#f92672">.</span>legend();

</code></pre></div><p><img src="https://user-images.githubusercontent.com/57588650/92989911-b306b880-f512-11ea-9dfe-b798728f260a.png" alt="output_5_0"></p>
<p>자기상관성을 이해하는 한 가지 방식은 &ldquo;내가 시간 $s$에서의 수열의 위치를 알 때, 이것이 시간 $t$에서의 내 위치를 아는데 도움이 될까?&ldquo;라는 질문을 하는 것입니다. 수열 $x_\tau$에서 이 질문의 답은 &ldquo;아니오&rdquo; 입니다. $x_\tau$를 만들 때, 그것은 확률 변수(random variable)입니다. 만일 내가 당신에게 $x_2 = 0.5$라고 말한다면 당신은 $x_3$에 대한 더 나은 추측을 할 수 있나요? 아니죠.</p>
<p>그러나 $y_t$는 자기상관성이 있습니다. 만일 내가 $y_2 = 10$이란 것을 안다면 $y_3$은 10 근처에 있을 것이라고 확신할 수 있기 때문이죠. 덜 확신하긴 하겠지만 $y_4$에 대한 추측 또한 할 수 있습니다. $y_4$가 0이나 20일 가능성은 거의 없지만, 5일 가능성은 그렇게 낮진 않죠. 같은 방식으로 $y_5$에 대한 추측도 할 수 있지만 역시 더 불확실할 것입니다. 이것을 논리적인 결론으로 만들면, 시간 지점 간의 차이(lag) $k$가 커질 수록 자기 상관성은 점점 줄어들게 됩니다. 이것을 시각화 해봅시다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">autocorr</span>(x):
    <span style="color:#75715e"># from http://tinyurl.com/afz57c4</span>
    result <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>correlate(x, x, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;full&#39;</span>)
    result <span style="color:#f92672">=</span> result <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>max(result)
    <span style="color:#66d9ef">return</span> result[result<span style="color:#f92672">.</span>size <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>:]

colors <span style="color:#f92672">=</span> [TFColor[<span style="color:#ae81ff">3</span>], TFColor[<span style="color:#ae81ff">0</span>], TFColor[<span style="color:#ae81ff">6</span>]]

x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">200</span>)
plt<span style="color:#f92672">.</span>bar(x, autocorr(y_t)[<span style="color:#ae81ff">1</span>:], width<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;$y_t$&#34;</span>,
        edgecolor<span style="color:#f92672">=</span>colors[<span style="color:#ae81ff">0</span>], color<span style="color:#f92672">=</span>colors[<span style="color:#ae81ff">0</span>])
plt<span style="color:#f92672">.</span>bar(x, autocorr(x_t)[<span style="color:#ae81ff">1</span>:], width<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;$x_t$&#34;</span>,
        color<span style="color:#f92672">=</span>colors[<span style="color:#ae81ff">1</span>], edgecolor<span style="color:#f92672">=</span>colors[<span style="color:#ae81ff">1</span>])

plt<span style="color:#f92672">.</span>legend(title<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Autocorrelation&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;$y_t$ 와 $y_{t-k}$</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74"> 사이의 상관관계&#34;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;k (시차)&#34;</span>)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;시차 $k$에 따라 달라지는 $y_t$와 $x_t$의 자기상관성 그래프&#34;</span>);
</code></pre></div><p><img src="https://user-images.githubusercontent.com/57588650/92989912-b39f4f00-f512-11ea-9173-9bb99394ff07.png" alt="output_7_0"></p>
<p>$y_t$의 자기상관성이 아주 높은 값에서 시작해 $k$가 높을 수록 점점 떨어진다는 것을 알 수 있습니다. $x_t$의 자기상관성을 비교해보면, $x_t$의 그래프는 잡음(noise)처럼 보입니다.(실제로도 그렇습니다.) 그렇기 때문에 $x_t$에는 자기상관성이 없다고 결론내릴 수 있습니다.</p>
<h2 id="어떻게-이것을-mcmc의-수렴성과-연관지을-수-있나요"><strong>어떻게 이것을 MCMC의 수렴성과 연관지을 수 있나요?</strong></h2>
<p>MCMC 알고리즘의 특성상 자기상관성을 보이는 표본들을 반환합니다.(이것은 단계가 지나면서 당신의 현재 위치에서 다음 위치로 이동하기 때문입니다.)</p>
<p>공간을 잘 탐색하지 못하는 체인은 아마도 아주 높은 자기상관성을 보일 것입니다. 시각적으로 트레이스가 강과 같이 꾸불꾸불하게 움직이고 한 곳에 머무르지 않는다면, 그 체인은 높은 자기상관성을 보이는 것입니다.</p>
<p>이것이 수렴한 MCMC가 항상 작은 자기상관성을 보인다는 뜻은 아닙니다. 그렇기 때문에 낮은 자기상관성은 수렴성의 필요조건은 아니지만 충분조건입니다. TFP는 내장된 자기상관성 툴을 가지고 있습니다.</p>
<h2 id="thinning"><strong>Thinning</strong></h2>
<p>또 다른 문제점은 사후 샘플끼리 높은 자기상관성이 있을 때 발생합니다. 많은 후처리 알고리즘(뽑은 샘플로 그 샘플이 뽑힌 분포의 모수를 추정하는 알고리즘)은 서로 독립인 샘플들을 필요로 합니다. 이것은 단지 n, 2n ,3n, .. 번째 표본을 반환해서 자기상관성을 줄임으로써 해결되거나 최소한 완화될 수 있습니다. 이 과정을 thinning이라고 하며 밑의 그래프에서 thinning의 정도에 따라 $y_t$의 자기상관성 그래프가 어떻게 달라지는지 보여드리겠습니다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">max_x <span style="color:#f92672">=</span> <span style="color:#ae81ff">200</span> <span style="color:#f92672">//</span> <span style="color:#ae81ff">3</span> <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">1</span>, max_x)

plt<span style="color:#f92672">.</span>bar(x, autocorr(y_t)[<span style="color:#ae81ff">1</span>:max_x], edgecolor<span style="color:#f92672">=</span>colors[<span style="color:#ae81ff">0</span>],
        label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;no thinning&#34;</span>, color<span style="color:#f92672">=</span>colors[<span style="color:#ae81ff">0</span>], width<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
plt<span style="color:#f92672">.</span>bar(x, autocorr(y_t[::<span style="color:#ae81ff">2</span>])[<span style="color:#ae81ff">1</span>:max_x], edgecolor<span style="color:#f92672">=</span>colors[<span style="color:#ae81ff">1</span>],
        label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2의 배수 번째 표본들만&#34;</span>, color<span style="color:#f92672">=</span>colors[<span style="color:#ae81ff">1</span>], width<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
plt<span style="color:#f92672">.</span>bar(x, autocorr(y_t[::<span style="color:#ae81ff">3</span>])[<span style="color:#ae81ff">1</span>:max_x], width<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, edgecolor<span style="color:#f92672">=</span>colors[<span style="color:#ae81ff">2</span>],
        label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;3의 배수 번째 표본들만&#34;</span>, color<span style="color:#f92672">=</span>colors[<span style="color:#ae81ff">2</span>])

plt<span style="color:#f92672">.</span>autoscale(tight<span style="color:#f92672">=</span>True)
plt<span style="color:#f92672">.</span>legend(title<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;$y_t$의 자기상관성 그래프&#34;</span>, loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;upper right&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;$y_t$ 와 $y_{t-k}$ 사이에서 측정된 자기상관성.&#34;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;k (시차)&#34;</span>)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;시차 $k$에 따라 달라지는 $y_t$의 자기상관성 (no thinning vs. thinning)&#34;</span>);
</code></pre></div><p><img src="https://user-images.githubusercontent.com/57588650/92989914-b4d07c00-f512-11ea-910d-e8ae424a7caf.png" alt="output_13_0"></p>
<p>더 많이 thinning할 수록 자기상관성이 더 빠르게 떨어지는 것을 볼 수 있습니다. 그러나 이것은 상충관계(trade-off)에 있습니다. 더 높은 수준의 thinning을 할 수록 같은 수의 표본을 얻기 위해 더 많은 MCMC 체인의 반복이 필요합니다. 예를 들어 thinning을 하지 않으면 10,000개의 샘플을 얻기 위해 10,000번의 반복을 하면 되지만 10의 thinning을 한다면 자기상관성은 더 낮겠지만 100,000번의 반복이 필요하게 됩니다.</p>
<p>적절한 thinning의 정도는 얼마나 될까요? 얼마나 많이 thinning을 하든 표본은 항상 약간의 자기상관성을 보일 것입니다. 그렇기 때문에 자기상관성이 0으로 가는 경향만 보여도 괜찮습니다. 일반적으로 10 이상의 thinning은 불필요합니다.</p>
<h2 id="mcmc할-때-유용한-팁들"><strong>MCMC할 때 유용한 팁들</strong></h2>
<p>베이지안 추론은 MCMC의 계산상의 어려움만 없었다면 표준적인 방법론이 되었을겁니다. 사실상 MCMC가 실용적인 베이지안 추론에 사람들이 싫증을 느끼게 만듭니다. 지금부터는 MCMC 엔진이 더 빠른 속도로 수렴하도록 하는 좋은 방법들을 알려주도록 하겠습니다.</p>
<h3 id="똑똑한-시작점"><strong>똑똑한 시작점</strong></h3>
<p>당연히 MCMC 알고리즘의 시작점을 사후 분포 근처에 두어야 올바른 샘플링을 하기 까지 짧은 시간이 걸립니다. <code>확률론적인(Stochastic)</code>변수를 만들 때 사후 분포가 어디 있을지에 대한 생각을 <code>testval</code> 변수에 넣음으로써 알고리즘을 더 좋게 만들 수 있습니다. 많은 케이스에서 우리는 모수가 무엇인지에 대한 합리적인 추론을 만들어낼 수 있습니다. 예를 들어 우리가 정규분포에서 온 데이터를 가지고 있고 모수 $\mu$를 알고싶다면, 좋은 시작점은 데이터의 평균이 될 것입니다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">mu <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Uniform(name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;mu&#39;</span>, low <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>, high <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>)<span style="color:#f92672">.</span>sample(seed <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>mean())
</code></pre></div><p>대부분의 모델에서의 모수는 그것의 빈도주의적 추정값이 있습니다. 이러한 추정값은 MCMC 알고리즘에서 좋은 시작점으로 사용됩니다. 당연히 모든 모수에 이것을 적용할 순 없겠지만, 최대한 많은 적절한 시작점을 설정하는 것은 항상 좋은 아이디어입니다. 만일 당신의 추측이 틀렸을지라도 MCMC는 여전히 적절한 분포로 수렴할 것이기 때문에 손해볼건 없기 때문이죠.</p>
<h3 id="사전-믿음"><strong>사전 믿음</strong></h3>
<p>만일 좋지 않은 사전 믿음이 설정되었을 경우, MCMC 알고리즘은 수렴하지 않거나 수렴하는데 어려움을 겪을 수도 있습니다. 만일 사전 믿음이 심지어는 실제 모수를 아예 포함하지 않은 경우 어떤 일이 일어날지 상상해봅시다. 만일 미지수에 사전 믿음이 0의 확률을 할당한디면 사후 확률도 그것에 0의 확률을 줄 것입니다. 이것이 이상한 결과를 내보낼 수도 있는 것이죠.</p>
<p>이 이유 때문에 사전 분포를 조심스럽게 정하는 것이 최선입니다. 수렴성의 부재나 표본들의 증거가 경계선에 모이는 현상은 무언가 잘못된 사전 분포가 선택되었다는 것을 의미합니다. 밑의 통계적 계산의 구전 이론을 봐봅시다.</p>
<blockquote>
<p>만일 당신이 계산적인 문제가 있다면, 당신의 모델이 잘못된 것이다.</p>
</blockquote>
<h2 id="결론"><strong>결론</strong></h2>
<p>TFP는 베이지안 추론을 실행하는데 아주 강력한 기반을 선사합니다. 왜냐하면 이것이 사용자들이 MCMC의 내부 작업들을 잘 수행할 수 있게 만들어주기 때문이죠.</p>
<h3 id="references"><strong>References</strong></h3>
<p>[1] Tensorflow Probability API docs. <a href="https://www.tensorflow.org/probability/api_docs/python/tfp">https://www.tensorflow.org/probability/api_docs/python/tfp</a></p>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/bayesian/" rel="tag">Bayesian</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/tensorflow/" rel="tag">TensorFlow</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/python/" rel="tag">Python</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div class="authorbox clearfix">
	<div class="authorbox__header">
		<span class="authorbox__name"></span>
	</div>
</div>



			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2021 Tai Hwan Oh.
			<span class="footer__copyright-credits"></span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>