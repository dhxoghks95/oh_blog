<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Bayesian Method with TensorFlow Chapter 2. More on TensorFlow and TensorFlow Probability - 5. 챌린저호 사고 예제 - Oh Data Science</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Bayesian Method with TensorFlow Chapter 2. More on TensorFlow and TensorFlow Probability - 5. 챌린저호 사고 예제" />
<meta property="og:description" content="Bayesian Method with TensorFlow - Chapter2 More on TensorFlow and TensorFlow Probability #@title Imports and Global Variables (make sure to run this cell) { display-mode: &#34;form&#34; } try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x except Exception: pass from __future__ import absolute_import, division, print_function #@markdown This sets the warning status (default is `ignore`, since this notebook runs correctly) warning_status = &#34;ignore&#34; #@param [&#34;ignore&#34;, &#34;always&#34;, &#34;module&#34;, &#34;once&#34;, &#34;default&#34;, &#34;error&#34;] import warnings warnings." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/challenger/" />
<meta property="article:published_time" content="2020-09-06T17:55:36+09:00" />
<meta property="article:modified_time" content="2020-09-06T17:55:36+09:00" />

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="Oh Data Science" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">Oh Data Science</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Bayesian Method with TensorFlow Chapter 2. More on TensorFlow and TensorFlow Probability - 5. 챌린저호 사고 예제</h1>
			
		</header><div class="content post__content clearfix">
			<h1 id="bayesian-method-with-tensorflow---chapter2-more-on-tensorflow-and-tensorflow-probability"><strong>Bayesian Method with TensorFlow - Chapter2 More on TensorFlow and TensorFlow Probability</strong></h1>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#@title Imports and Global Variables (make sure to run this cell)  { display-mode: &#34;form&#34; }</span>

<span style="color:#66d9ef">try</span>:
  <span style="color:#75715e"># %tensorflow_version only exists in Colab.</span>
  <span style="color:#f92672">%</span>tensorflow_version <span style="color:#ae81ff">2.</span>x
<span style="color:#66d9ef">except</span> <span style="color:#a6e22e">Exception</span>:
  <span style="color:#66d9ef">pass</span>


<span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> absolute_import, division, print_function


<span style="color:#75715e">#@markdown This sets the warning status (default is `ignore`, since this notebook runs correctly)</span>
warning_status <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;ignore&#34;</span> <span style="color:#75715e">#@param [&#34;ignore&#34;, &#34;always&#34;, &#34;module&#34;, &#34;once&#34;, &#34;default&#34;, &#34;error&#34;]</span>
<span style="color:#f92672">import</span> warnings
warnings<span style="color:#f92672">.</span>filterwarnings(warning_status)
<span style="color:#66d9ef">with</span> warnings<span style="color:#f92672">.</span>catch_warnings():
    warnings<span style="color:#f92672">.</span>filterwarnings(warning_status, category<span style="color:#f92672">=</span><span style="color:#a6e22e">DeprecationWarning</span>)
    warnings<span style="color:#f92672">.</span>filterwarnings(warning_status, category<span style="color:#f92672">=</span><span style="color:#a6e22e">UserWarning</span>)

<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> os
<span style="color:#75715e">#@markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/)</span>
matplotlib_style <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;fivethirtyeight&#39;</span> <span style="color:#75715e">#@param [&#39;fivethirtyeight&#39;, &#39;bmh&#39;, &#39;ggplot&#39;, &#39;seaborn&#39;, &#39;default&#39;, &#39;Solarize_Light2&#39;, &#39;classic&#39;, &#39;dark_background&#39;, &#39;seaborn-colorblind&#39;, &#39;seaborn-notebook&#39;]</span>
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt; plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(matplotlib_style)
<span style="color:#f92672">import</span> matplotlib.axes <span style="color:#f92672">as</span> axes;
<span style="color:#f92672">from</span> matplotlib.patches <span style="color:#f92672">import</span> Ellipse
<span style="color:#f92672">import</span> matplotlib <span style="color:#f92672">as</span> mpl
<span style="color:#75715e">#%matplotlib inline</span>
<span style="color:#f92672">import</span> seaborn <span style="color:#f92672">as</span> sns; sns<span style="color:#f92672">.</span>set_context(<span style="color:#e6db74">&#39;notebook&#39;</span>)
<span style="color:#f92672">from</span> IPython.core.pylabtools <span style="color:#f92672">import</span> figsize
<span style="color:#75715e">#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)</span>
notebook_screen_res <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;retina&#39;</span> <span style="color:#75715e">#@param [&#39;retina&#39;, &#39;png&#39;, &#39;jpeg&#39;, &#39;svg&#39;, &#39;pdf&#39;]</span>
<span style="color:#75715e">#%config InlineBackend.figure_format = notebook_screen_res</span>

<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf

<span style="color:#f92672">import</span> tensorflow_probability <span style="color:#f92672">as</span> tfp
tfd <span style="color:#f92672">=</span> tfp<span style="color:#f92672">.</span>distributions
tfb <span style="color:#f92672">=</span> tfp<span style="color:#f92672">.</span>bijectors

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">_TFColor</span>(object):
    <span style="color:#e6db74">&#34;&#34;&#34;Enum of colors used in TF docs.&#34;&#34;&#34;</span>
    red <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#F15854&#39;</span>
    blue <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#5DA5DA&#39;</span>
    orange <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#FAA43A&#39;</span>
    green <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#60BD68&#39;</span>
    pink <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#F17CB0&#39;</span>
    brown <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#B2912F&#39;</span>
    purple <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#B276B2&#39;</span>
    yellow <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#DECF3F&#39;</span>
    gray <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#4D4D4D&#39;</span>
    <span style="color:#66d9ef">def</span> __getitem__(self, i):
        <span style="color:#66d9ef">return</span> [
            self<span style="color:#f92672">.</span>red,
            self<span style="color:#f92672">.</span>orange,
            self<span style="color:#f92672">.</span>green,
            self<span style="color:#f92672">.</span>blue,
            self<span style="color:#f92672">.</span>pink,
            self<span style="color:#f92672">.</span>brown,
            self<span style="color:#f92672">.</span>purple,
            self<span style="color:#f92672">.</span>yellow,
            self<span style="color:#f92672">.</span>gray,
        ][i <span style="color:#f92672">%</span> <span style="color:#ae81ff">9</span>]
TFColor <span style="color:#f92672">=</span> _TFColor()

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">session_options</span>(enable_gpu_ram_resizing<span style="color:#f92672">=</span>True, enable_xla<span style="color:#f92672">=</span>False):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Allowing the notebook to make use of GPUs if they&#39;re available.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear
</span><span style="color:#e6db74">    algebra that optimizes TensorFlow computations.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    config <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>config
    gpu_devices <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>list_physical_devices(<span style="color:#e6db74">&#39;GPU&#39;</span>)
    <span style="color:#66d9ef">if</span> enable_gpu_ram_resizing:
        <span style="color:#66d9ef">for</span> device <span style="color:#f92672">in</span> gpu_devices:
           tf<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>set_memory_growth(device, True)
    <span style="color:#66d9ef">if</span> enable_xla:
        config<span style="color:#f92672">.</span>optimizer<span style="color:#f92672">.</span>set_jit(True)
    <span style="color:#66d9ef">return</span> config

session_options(enable_gpu_ram_resizing<span style="color:#f92672">=</span>True, enable_xla<span style="color:#f92672">=</span>True)

<span style="color:#960050;background-color:#1e0010">!</span>apt <span style="color:#f92672">-</span>qq <span style="color:#f92672">-</span>y install fonts<span style="color:#f92672">-</span>nanum
 
<span style="color:#f92672">import</span> matplotlib.font_manager <span style="color:#f92672">as</span> fm
fontpath <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf&#39;</span>
font <span style="color:#f92672">=</span> fm<span style="color:#f92672">.</span>FontProperties(fname<span style="color:#f92672">=</span>fontpath, size<span style="color:#f92672">=</span><span style="color:#ae81ff">9</span>)
plt<span style="color:#f92672">.</span>rc(<span style="color:#e6db74">&#39;font&#39;</span>, family<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;NanumBarunGothic&#39;</span>) 
mpl<span style="color:#f92672">.</span>font_manager<span style="color:#f92672">.</span>_rebuild()
</code></pre></div><pre><code>fonts-nanum is already the newest version (20170925-1).
The following package was automatically installed and is no longer required:
  libnvidia-common-440
Use 'apt autoremove' to remove it.
0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.
</code></pre>
<h2 id="5-예제--우주왕복선-챌린저호-사고"><strong>5. 예제 : 우주왕복선 챌린저호 사고</strong></h2>
<p>1986년 1월 28일에 미국의 25번째 우주 왕복선 챌린저호는 발사 직후 로켓 중 하나가 폭발하고 7명의 승무원이 모두 사망하는 참사로 끝나고 말았습니다. 대통령 직속 사고 조사 위원회는 이 사고가 로켓에 연결된 O-ring의 실패로 인해 발생했으며 이는 O-ring이 외부 기온을 포함한 다양한 요인에 과도하게 민감하게 잘못 설계되었기 때문이라고 결론지었습니다. 이전 24번의 비행에서, O-ring의 실패에 대한 데이터를 23번이나 얻을 수 있었습니다(1번은 바다에서 유실되었습니다.). 그리고 이 데이터들은 챌린저호가 발사되기 전 저녁에 논의되었죠. 그러나 불행하게도 오직 손상이 발생했던 7건의 비행만이 중요하게 여겨졌고 이들은 명확한 추이가 없다고 생각되었습니다. 데이터는 다음과 같습니다.(주석 [1]을 보세요):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># pip install wget</span>
<span style="color:#f92672">import</span> wget
url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter2_MorePyMC/data/challenger_data.csv&#39;</span>
filename <span style="color:#f92672">=</span> wget<span style="color:#f92672">.</span>download(url)
filename
</code></pre></div><pre><code>'challenger_data (2).csv'
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt<span style="color:#f92672">.</span>figure(figsize(<span style="color:#ae81ff">12.5</span>, <span style="color:#ae81ff">3.5</span>))
np<span style="color:#f92672">.</span>set_printoptions(precision<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, suppress<span style="color:#f92672">=</span>True)
challenger_data_ <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>genfromtxt(<span style="color:#e6db74">&#34;challenger_data.csv&#34;</span>, skip_header<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
                                usecols<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>], missing_values<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;NA&#34;</span>,
                                delimiter<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;,&#34;</span>)
<span style="color:#75715e">#drop the NA values</span>
challenger_data_ <span style="color:#f92672">=</span> challenger_data_[<span style="color:#f92672">~</span>np<span style="color:#f92672">.</span>isnan(challenger_data_[:, <span style="color:#ae81ff">1</span>])]

<span style="color:#75715e">#plot it, as a function of tempature (the first column)</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;기온 (F), O-ring이 실패했는가?&#34;</span>)
<span style="color:#66d9ef">print</span>(challenger_data_)

plt<span style="color:#f92672">.</span>scatter(challenger_data_[:, <span style="color:#ae81ff">0</span>], challenger_data_[:, <span style="color:#ae81ff">1</span>], s<span style="color:#f92672">=</span><span style="color:#ae81ff">75</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;k&#34;</span>,
            alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
plt<span style="color:#f92672">.</span>yticks([<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>])
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;손상이 발생했는가?&#34;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;외부기온 (화씨)&#34;</span>)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;우주왕복선 O-ring의 손상 vs 외부 기온&#34;</span>);

</code></pre></div><pre><code>기온 (F), O-ring이 실패했는가?
[[66.  0.]
 [70.  1.]
 [69.  0.]
 [68.  0.]
 [67.  0.]
 [72.  0.]
 [73.  0.]
 [70.  0.]
 [57.  1.]
 [63.  1.]
 [70.  1.]
 [78.  0.]
 [67.  0.]
 [53.  1.]
 [67.  0.]
 [75.  0.]
 [70.  0.]
 [81.  0.]
 [76.  0.]
 [79.  0.]
 [75.  1.]
 [76.  0.]
 [58.  1.]]
</code></pre>
<p><img src="https://user-images.githubusercontent.com/57588650/92322147-7f3b1700-f06a-11ea-8bc3-01c59787db4b.png" alt="output_5_1"></p>
<p>외부 기온이 낮을 수록 손상이 발생할 확률이 높아진다는것이 명확하게 보입니다. 여기에 손상 발생과 외부 기온 사이에 완벽하게 나눠떨어지는 구분점은 없어 보이기 때문에 여기에서는 손상 확률을 모델링하도록 하겠습니다. 최선의 질문은 이겁니다. &ldquo;기온 $t$에서, 손상이 발행할 확률은 어떻습니까?&rdquo;. 이 예제의 목표는 바로 이 질문에 답을 하는거죠.</p>
<p>우리는 $p(t)$라고 불리는 온도 $t$에 대한 함수가 필요합니다. 이 함수는 확률을 모델링하기 위한 것이기 때문에 0과 1 사이의 값을 가지고 온도가 올라갈수록 1에서 0으로 갑니다. 그러한 함수들은 많은데, 가장 유명한 것은 로지스틱 함수입니다.</p>
<p>$$p(t) = \frac{1}{ 1 + e^{ ;\beta t } } $$</p>
<p>이 모델에서 $\beta$는 우리가 모르는 변수입니다. $\beta$가 1, 3, -5일 때의 그래프를 그려보도록 하겠습니다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># evaluate 함수 생성</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">evaluate</span>(tensors):
    <span style="color:#66d9ef">if</span> tf<span style="color:#f92672">.</span>executing_eagerly():
         <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>nest<span style="color:#f92672">.</span>pack_sequence_as(
             tensors,
             [t<span style="color:#f92672">.</span>numpy() <span style="color:#66d9ef">if</span> tf<span style="color:#f92672">.</span>is_tensor(t) <span style="color:#66d9ef">else</span> t
             <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tf<span style="color:#f92672">.</span>nest<span style="color:#f92672">.</span>flatten(tensors)])
    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>Session() <span style="color:#66d9ef">as</span> sess:
        <span style="color:#66d9ef">return</span> sess<span style="color:#f92672">.</span>run(tensors)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">logistic</span>(x, beta):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    로지스틱 함수
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">    Args:
</span><span style="color:#e6db74">      x: 독립 변수
</span><span style="color:#e6db74">      beta: 베타 항
</span><span style="color:#e6db74">    Returns: 
</span><span style="color:#e6db74">      로지스틱 함수
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">+</span> tf<span style="color:#f92672">.</span>exp(beta <span style="color:#f92672">*</span> x))

<span style="color:#75715e"># x값들을 -4부터 4까지 100개 만듦</span>
x_vals <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>linspace(start<span style="color:#f92672">=-</span><span style="color:#ae81ff">4.</span>, stop<span style="color:#f92672">=</span><span style="color:#ae81ff">4.</span>, num<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)

<span style="color:#75715e"># 만들어진 x를 투입해 로지스틱 함수 만들기</span>
log_beta_1 <span style="color:#f92672">=</span> logistic(x_vals, <span style="color:#ae81ff">1.</span>)
log_beta_3 <span style="color:#f92672">=</span> logistic(x_vals, <span style="color:#ae81ff">3.</span>)
log_beta_m5 <span style="color:#f92672">=</span> logistic(x_vals, <span style="color:#f92672">-</span><span style="color:#ae81ff">5.</span>)

<span style="color:#75715e"># 만들어진 로지스틱 함수 실행</span>
[
    x_vals_,
    log_beta_1_,
    log_beta_3_,
    log_beta_m5_,
] <span style="color:#f92672">=</span> evaluate([
    x_vals,
    log_beta_1,
    log_beta_3,
    log_beta_m5,
])

<span style="color:#75715e"># 그래프 그리기</span>
plt<span style="color:#f92672">.</span>figure(figsize(<span style="color:#ae81ff">12.5</span>, <span style="color:#ae81ff">3</span>))
plt<span style="color:#f92672">.</span>plot(x_vals_, log_beta_1_, label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;$\beta = 1$&#34;</span>, color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">0</span>])
plt<span style="color:#f92672">.</span>plot(x_vals_, log_beta_3_, label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;$\beta = 3$&#34;</span>, color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">3</span>])
plt<span style="color:#f92672">.</span>plot(x_vals_, log_beta_m5_, label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;$\beta = -5$&#34;</span>, color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">6</span>])
plt<span style="color:#f92672">.</span>legend();
</code></pre></div><p><img src="https://user-images.githubusercontent.com/57588650/92322155-895d1580-f06a-11ea-9876-0a89c642d689.png" alt="output_7_0"></p>
<p>빠진 것이 있죠? 바로 확률이 오직 0 근처에서만 변동이 있는데 위의 챌린저호 데이터에선 65에서 70 사이에서 확률이 변동한다는 것입니다. 그래서 우리의 로지스틱 함수에 편향(bias)을 추가하기 위해 $\alpha$를 넣도록 하겠습니다.</p>
<p>$$p(t) = \frac{1}{ 1 + e^{ ;\beta t + \alpha } } $$</p>
<p>다음은 $\alpha$를 추가한 후의 그래프 입니다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">logistic</span>(x, beta, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Intercept가 있는 로지스틱 함수
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">    Args:
</span><span style="color:#e6db74">        x: 독립 변수
</span><span style="color:#e6db74">        beta: 베타 항 
</span><span style="color:#e6db74">        alpha: 알파 항
</span><span style="color:#e6db74">    Returns: 
</span><span style="color:#e6db74">        로지스틱 함수
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">+</span> tf<span style="color:#f92672">.</span>exp((beta <span style="color:#f92672">*</span> x) <span style="color:#f92672">+</span> alpha))

<span style="color:#75715e"># beta = 1, 3, -5 / alpha = 1, -2, 7 일 때의 그래프를 그려보겠습니다</span>
x_vals <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>linspace(start<span style="color:#f92672">=-</span><span style="color:#ae81ff">4.</span>, stop<span style="color:#f92672">=</span><span style="color:#ae81ff">4.</span>, num<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)
log_beta_1_alpha_1 <span style="color:#f92672">=</span> logistic(x_vals, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
log_beta_3_alpha_m2 <span style="color:#f92672">=</span> logistic(x_vals, <span style="color:#ae81ff">3</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>)
log_beta_m5_alpha_7 <span style="color:#f92672">=</span> logistic(x_vals, <span style="color:#f92672">-</span><span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">7</span>)

<span style="color:#75715e"># 실행합시다</span>
[
    x_vals_,
    log_beta_1_alpha_1_,
    log_beta_3_alpha_m2_,
    log_beta_m5_alpha_7_,
] <span style="color:#f92672">=</span> evaluate([
    x_vals,
    log_beta_1_alpha_1,
    log_beta_3_alpha_m2,
    log_beta_m5_alpha_7,
])

<span style="color:#75715e"># 그래프를 그립시다</span>
plt<span style="color:#f92672">.</span>figure(figsize(<span style="color:#ae81ff">12.5</span>, <span style="color:#ae81ff">3</span>))
plt<span style="color:#f92672">.</span>plot(x_vals_, log_beta_1_, label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;$\beta = 1$&#34;</span>, ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;--&#34;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">0</span>])
plt<span style="color:#f92672">.</span>plot(x_vals_, log_beta_3_, label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;$\beta = 3$&#34;</span>, ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;--&#34;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">3</span>])
plt<span style="color:#f92672">.</span>plot(x_vals_, log_beta_m5_, label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;$\beta = -5$&#34;</span>, ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;--&#34;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">6</span>])
plt<span style="color:#f92672">.</span>plot(x_vals_, log_beta_1_alpha_1_, label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;$\beta = 1, \alpha = 1$&#34;</span>, color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">0</span>])
plt<span style="color:#f92672">.</span>plot(x_vals_, log_beta_3_alpha_m2_, label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;$\beta = 3, \alpha = -2$&#34;</span>, color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">3</span>])
plt<span style="color:#f92672">.</span>plot(x_vals_, log_beta_m5_alpha_7_, label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;$\beta = -5, \alpha = 7$&#34;</span>, color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">6</span>])
plt<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lower left&#34;</span>);
</code></pre></div><p><img src="https://user-images.githubusercontent.com/57588650/92322165-92e67d80-f06a-11ea-9ba9-184fa5db8ebe.png" alt="output_9_0"></p>
<p>상수항 $\alpha$를 넣음에 따라 곡선이 왼쪽 오른쪽으로 이동합니다.(이것이 편향이라고 불리는 이유죠)</p>
<p>자 이제 이것을 TFP에서 모델링해봅시다. $\beta, \alpha$는 양수일 필요도, 상한과 하한이 있지도 상대적으로 크지도 않습니다. 그래서 다음에 소개할 정규 확률 변수(normal random variable)로 최선의 모델을 만들 수 있습니다.</p>
<h3 id="정규-분포"><strong>정규 분포</strong></h3>
<p>$X \sim N(\mu, 1/\tau)$로 나타내어지는 정규 확률 변수는 평균인 $\mu$와 precision $\tau$ 두 개를 모수로 가집니다. 여러분이 보통 알고있는 정규 분포의 $\sigma^2$ 대신에 $\tau^-1$를 쓰겠습니다. 그들은 사실 단지 서로의 역수일 뿐입니다. 이렇게 바꾸는 이유는 더 쉬운 수학적 분석을 할 수 있고 그것이 오래된 베이지안 방법론의 유물이기 때문입니다. 그냥 $\tau$가 작아질 수록 분포가 더 넓게 퍼지고(더 불확실해지고) 커질 수록 분포가 더 뾰족해진다(더 확실해진다)는 것만 기억하세요. 어쨌든 $\tau$는 항상 양수입니다.</p>
<p>$N(\mu, 1/\tau)$의 확률 밀도 함수(probability density fucntion)은 다음과 같습니다.</p>
<p>$$ f(x | \mu, \tau) = \sqrt{\frac{\tau}{2\pi}} \exp\left( -\frac{\tau}{2} (x-\mu)^2 \right) $$</p>
<p>자 이제 몇 개의 예시를 그래프로 그려보겠습니다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># x의 범위를 -8부터 7까지로 합시다</span>
rand_x_vals <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>linspace(start<span style="color:#f92672">=-</span><span style="color:#ae81ff">8.</span>, stop<span style="color:#f92672">=</span><span style="color:#ae81ff">7.</span>, num<span style="color:#f92672">=</span><span style="color:#ae81ff">150</span>)

<span style="color:#75715e"># N(-2, 1/7), N(0, 1), N(3, 1/2.8)을 그려보도록 합시다</span>
density_func_1 <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Normal(loc<span style="color:#f92672">=</span>float(<span style="color:#f92672">-</span><span style="color:#ae81ff">2.</span>), scale<span style="color:#f92672">=</span>float(<span style="color:#ae81ff">1.</span><span style="color:#f92672">/.</span><span style="color:#ae81ff">7</span>))<span style="color:#f92672">.</span>prob(rand_x_vals)
density_func_2 <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Normal(loc<span style="color:#f92672">=</span>float(<span style="color:#ae81ff">0.</span>), scale<span style="color:#f92672">=</span>float(<span style="color:#ae81ff">1.</span><span style="color:#f92672">/</span><span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>prob(rand_x_vals)
density_func_3 <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Normal(loc<span style="color:#f92672">=</span>float(<span style="color:#ae81ff">3.</span>), scale<span style="color:#f92672">=</span>float(<span style="color:#ae81ff">1.</span><span style="color:#f92672">/</span><span style="color:#ae81ff">2.8</span>))<span style="color:#f92672">.</span>prob(rand_x_vals)

<span style="color:#75715e">#그래프를 실행합니다</span>
[
    rand_x_vals_,
    density_func_1_,
    density_func_2_,
    density_func_3_,
] <span style="color:#f92672">=</span> evaluate([
    rand_x_vals,
    density_func_1,
    density_func_2,
    density_func_3,
])

colors <span style="color:#f92672">=</span> [TFColor[<span style="color:#ae81ff">3</span>], TFColor[<span style="color:#ae81ff">0</span>], TFColor[<span style="color:#ae81ff">6</span>]]

plt<span style="color:#f92672">.</span>figure(figsize(<span style="color:#ae81ff">12.5</span>, <span style="color:#ae81ff">3</span>))
plt<span style="color:#f92672">.</span>plot(rand_x_vals_, density_func_1_,
         label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;$\mu = </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">, \tau = </span><span style="color:#e6db74">%.1f</span><span style="color:#e6db74">$&#34;</span> <span style="color:#f92672">%</span> (<span style="color:#f92672">-</span><span style="color:#ae81ff">2.</span>, <span style="color:#f92672">.</span><span style="color:#ae81ff">7</span>), color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">3</span>])
plt<span style="color:#f92672">.</span>fill_between(rand_x_vals_, density_func_1_, color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">3</span>], alpha<span style="color:#f92672">=.</span><span style="color:#ae81ff">33</span>)
plt<span style="color:#f92672">.</span>plot(rand_x_vals_, density_func_2_, 
         label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;$\mu = </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">, \tau = </span><span style="color:#e6db74">%.1f</span><span style="color:#e6db74">$&#34;</span> <span style="color:#f92672">%</span> (<span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">1</span>), color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">0</span>])
plt<span style="color:#f92672">.</span>fill_between(rand_x_vals_, density_func_2_, color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">0</span>], alpha<span style="color:#f92672">=.</span><span style="color:#ae81ff">33</span>)
plt<span style="color:#f92672">.</span>plot(rand_x_vals_, density_func_3_,
         label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;$\mu = </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">, \tau = </span><span style="color:#e6db74">%.1f</span><span style="color:#e6db74">$&#34;</span> <span style="color:#f92672">%</span> (<span style="color:#ae81ff">3.</span>, <span style="color:#ae81ff">2.8</span>), color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">6</span>])
plt<span style="color:#f92672">.</span>fill_between(rand_x_vals_, density_func_3_, color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">6</span>], alpha<span style="color:#f92672">=.</span><span style="color:#ae81ff">33</span>)

plt<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;upper right&#34;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;$x$&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;$x$에서의 밀도 함수&#34;</span>)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;세 가지 다른 정규 확률 변수의 확률 분포&#34;</span>);
</code></pre></div><p><img src="https://user-images.githubusercontent.com/57588650/92322174-9bd74f00-f06a-11ea-95f8-0086e472d127.png" alt="output_13_0"></p>
<p>정규 확률 변수는 어떤 실수도 값으로 가질 수 있지만, 변수들이 $\mu$ 근처에 몰려있을 확률이 높습니다. 실제로 정규 확률 변수의 기댓값은 그것의 모수 $\mu$와 같습니다.</p>
<p>$$ E[ X | \mu, \tau] = \mu$$</p>
<p>그리고 그것의 분산은 $\tau$의 역수와 같죠.</p>
<p>$$\text{Var}( X | \mu, \tau ) = \frac{1}{\tau}$$</p>
<p>자 이제 챌린저호 모델링을 계속 해봅시다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># 외부 기온을 텐서로 만들기</span>
temperature_ <span style="color:#f92672">=</span> challenger_data_[:, <span style="color:#ae81ff">0</span>]
temperature <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>convert_to_tensor(temperature_, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)

<span style="color:#75715e"># 손상 여부를 텐서로 만들기</span>
D_ <span style="color:#f92672">=</span> challenger_data_[:, <span style="color:#ae81ff">1</span>]                <span style="color:#75715e"># defect or not?</span>
D <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>convert_to_tensor(D_, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)

<span style="color:#75715e"># beta와 alpha를 Normal 분포에서 샘플링</span>
beta <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Normal(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;beta&#34;</span>, loc<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>, scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1000.</span>)<span style="color:#f92672">.</span>sample()
alpha <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Normal(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;alpha&#34;</span>, loc<span style="color:#f92672">=-</span><span style="color:#ae81ff">15.</span>, scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1000.</span>)<span style="color:#f92672">.</span>sample()

<span style="color:#75715e"># beta와 온도, alpha를 로지스틱 함수로 합쳐서 결정론적인 확률 만들기</span>
p_deterministic <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Deterministic(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;p&#34;</span>, loc<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span><span style="color:#f92672">/</span>(<span style="color:#ae81ff">1.</span> <span style="color:#f92672">+</span> tf<span style="color:#f92672">.</span>exp(beta <span style="color:#f92672">*</span> temperature_ <span style="color:#f92672">+</span> alpha)))<span style="color:#f92672">.</span>sample()

<span style="color:#75715e"># 그래프 실행하기</span>
[
    prior_alpha_,
    prior_beta_,
    p_deterministic_,
    D_,
] <span style="color:#f92672">=</span> evaluate([
    alpha,
    beta,
    p_deterministic,
    D,
])

</code></pre></div><p>이제 확률을 만들긴 했는데 어떻게 그들을 관측치들과 연결해야 할까요? $p$를 모수로 가지는 베르누이 확률 변수는 $p$의 확률로 1의 값을 가지고 나머지는 0입니다. 그래서 우리의 모델은 다음과 같이 쓸 수 있죠</p>
<p>$$ \text{Defect Incident, }D_i \sim \text{Ber}( p(t_i) ), i=1..N$$</p>
<p>여기서 $p(t)$는 우리의 로지스틱 함수이고 $t_i$는 우리가 관측한 온도입니다. 밑의 코드에서 우리는 <code>beta</code>와 <code>alpha</code>의 값을 <code>initial_chain_state</code>에서 0으로 두고 시작한다는 점에 주목합시다. 왜냐하면 만일 <code>beta</code>나 <code>alpha</code>가 아주 크다면 그들은 <code>p</code>를 1 또는 0으로 만들 것이기 때문입니다. 불행하게도 <code>tfd.Bernoulli</code>는 0이나 1로 정확히 떨어지는 확률을 그들이 수학적으로는 잘 정의된 것임에도 불구하고 별로 좋아하지 않습니다. 그래서 <code>alpha</code>와 <code>beta</code>를 0으로 설정함으로서 합리적인 시작점에서 시작하게 합니다. 이것은 우리의 결과에는 아무련 영향도 없고 우리의 사전 믿음에 추가적인 정보를 넣는 것도 아닙니다. 단지 TFP에서 계산할 때 이렇게 하라고 해서 하는거죠. 자 이제 우리가 지금까지 한 방식대로 베이지안 추론을 해봅시다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># joint_log_prob 함수를 만듭시다</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">challenger_joint_log_prob</span>(D, temperature_, alpha, beta):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    결합 로그 확률 최적화 함수
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">    Args:
</span><span style="color:#e6db74">      D: 결함이 나타났는지 안나타났는지를 보여주는 챌린저 참사 데이터
</span><span style="color:#e6db74">      temperature_: 결함이 나타났을 때 또는 나타나지 않았을 때의 기온을 나타내는 챌린저 참사 데이터
</span><span style="color:#e6db74">      alpha: HMC에 넣을 투입값 중 하나
</span><span style="color:#e6db74">      beta: HMC에 넣을 투입값 중 하나
</span><span style="color:#e6db74">    Returns: 
</span><span style="color:#e6db74">      결합 로그 확률 최적화 함수
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>

    <span style="color:#75715e"># N(0, 1000) 으로 시작합시다</span>
    rv_alpha <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Normal(loc<span style="color:#f92672">=</span><span style="color:#ae81ff">0.</span>, scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1000.</span>)
    rv_beta <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Normal(loc<span style="color:#f92672">=</span><span style="color:#ae81ff">0.</span>, scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1000.</span>)

    <span style="color:#75715e"># 이것을 로지스틱 함수로 변환합시다</span>
    logistic_p <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span><span style="color:#f92672">/</span>(<span style="color:#ae81ff">1.</span> <span style="color:#f92672">+</span> tf<span style="color:#f92672">.</span>exp(beta <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>cast(temperature_, tf<span style="color:#f92672">.</span>float32) <span style="color:#f92672">+</span> alpha))
    rv_observed <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Bernoulli(probs<span style="color:#f92672">=</span>logistic_p)
    
    <span style="color:#66d9ef">return</span> (
        rv_alpha<span style="color:#f92672">.</span>log_prob(alpha)
        <span style="color:#f92672">+</span> rv_beta<span style="color:#f92672">.</span>log_prob(beta)
        <span style="color:#f92672">+</span> tf<span style="color:#f92672">.</span>reduce_sum(rv_observed<span style="color:#f92672">.</span>log_prob(D))
    )
</code></pre></div><p>HMC 모델을 만들고 돌려봅시다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">number_of_steps <span style="color:#f92672">=</span> <span style="color:#ae81ff">10000</span> 
burnin <span style="color:#f92672">=</span> <span style="color:#ae81ff">2000</span> 

<span style="color:#75715e"># 체인의 시작점을 alpha = 0, beta = 0으로 설정합시다</span>
initial_chain_state <span style="color:#f92672">=</span> [
    <span style="color:#ae81ff">0.</span> <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>ones([], dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;init_alpha&#34;</span>),
    <span style="color:#ae81ff">0.</span> <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>ones([], dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;init_beta&#34;</span>)
]

<span style="color:#75715e"># HMC가 과도하게 제약이 없으므로 실제 값으로 나오게 하기 위해 변환합니다</span>
<span style="color:#75715e"># 챌린저호 문제에서 대략 alpha가 beta의 100배이기 때문에 그것을 AffineScalar bijctor를 통해 반영합니다</span>

unconstraining_bijectors <span style="color:#f92672">=</span> [
    tfp<span style="color:#f92672">.</span>bijectors<span style="color:#f92672">.</span>AffineScalar(<span style="color:#ae81ff">100.</span>),
    tfp<span style="color:#f92672">.</span>bijectors<span style="color:#f92672">.</span>Identity()
]

<span style="color:#75715e"># 우리의 joint_log_prob에 대해 클로저를 정의합니다</span>
unnormalized_posterior_log_prob <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> <span style="color:#f92672">*</span>args: challenger_joint_log_prob(D, temperature_, <span style="color:#f92672">*</span>args)

<span style="color:#75715e"># step_size를 정의합니다.</span>
step_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>

<span style="color:#75715e"># HMC를 정의합니다</span>
hmc<span style="color:#f92672">=</span>tfp<span style="color:#f92672">.</span>mcmc<span style="color:#f92672">.</span>TransformedTransitionKernel(
    inner_kernel<span style="color:#f92672">=</span>tfp<span style="color:#f92672">.</span>mcmc<span style="color:#f92672">.</span>HamiltonianMonteCarlo(
        target_log_prob_fn<span style="color:#f92672">=</span>unnormalized_posterior_log_prob,
        num_leapfrog_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">40</span>, <span style="color:#75715e">#to improve convergence</span>
        step_size<span style="color:#f92672">=</span>step_size,
        state_gradients_are_stopped<span style="color:#f92672">=</span>True),
    bijector<span style="color:#f92672">=</span>unconstraining_bijectors)

hmc <span style="color:#f92672">=</span> tfp<span style="color:#f92672">.</span>mcmc<span style="color:#f92672">.</span>SimpleStepSizeAdaptation(inner_kernel<span style="color:#f92672">=</span>hmc, num_adaptation_steps<span style="color:#f92672">=</span>int(burnin <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.8</span>))

<span style="color:#75715e"># Sampling from the chain.</span>
[
    posterior_alpha,
    posterior_beta
], kernel_results <span style="color:#f92672">=</span> tfp<span style="color:#f92672">.</span>mcmc<span style="color:#f92672">.</span>sample_chain(
    num_results <span style="color:#f92672">=</span> number_of_steps,
    num_burnin_steps <span style="color:#f92672">=</span> burnin,
    current_state<span style="color:#f92672">=</span>initial_chain_state,
    kernel<span style="color:#f92672">=</span>hmc)


<span style="color:#75715e">### **사후 분포에서 샘플링하기 위해 TF graph를 실행합시다**</span>


<span style="color:#e6db74">``</span><span style="color:#960050;background-color:#1e0010">`</span>python
<span style="color:#f92672">%%</span>time
<span style="color:#75715e"># 그래프 모드에서 이것은 15분 이상 걸릴 수도 있습니다.</span>

[
    posterior_alpha_,
    posterior_beta_,
    kernel_results_
] <span style="color:#f92672">=</span> evaluate([
    posterior_alpha,
    posterior_beta,
    kernel_results
])
</code></pre></div><pre><code>CPU times: user 2.74 ms, sys: 0 ns, total: 2.74 ms
Wall time: 2.2 ms
</code></pre>
<p>관측값에 대해 우리들의 모델을 학습 완료했으니까 alpha와 beta의 사후 분포가 어떤지 봐봅시다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt<span style="color:#f92672">.</span>figure(figsize(<span style="color:#ae81ff">12.5</span>, <span style="color:#ae81ff">6</span>))

<span style="color:#75715e">#샘플들의 사후분포</span>
plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">211</span>)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;Posterior distributions of the variables $\alpha, \beta$&#34;</span>)
plt<span style="color:#f92672">.</span>hist(posterior_beta_, histtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;stepfilled&#39;</span>, bins<span style="color:#f92672">=</span><span style="color:#ae81ff">35</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.85</span>,
         label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;posterior of $\beta$&#34;</span>, color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">6</span>], density<span style="color:#f92672">=</span>True)
plt<span style="color:#f92672">.</span>legend()

plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">212</span>)
plt<span style="color:#f92672">.</span>hist(posterior_alpha_, histtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;stepfilled&#39;</span>, bins<span style="color:#f92672">=</span><span style="color:#ae81ff">35</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.85</span>,
         label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;posterior of $\alpha$&#34;</span>, color<span style="color:#f92672">=</span>TFColor[<span style="color:#ae81ff">0</span>], density<span style="color:#f92672">=</span>True)
plt<span style="color:#f92672">.</span>legend();
</code></pre></div><p><img src="https://user-images.githubusercontent.com/57588650/92322186-ba3d4a80-f06a-11ea-8ef0-2147c7ff6022.png" alt="output_24_0"></p>
<p>모든 $\beta$의 샘플들이 0보다 큰 것을 알 수 있습니다. 만약 기온이 손상 확률에 아무런 영향이 없다면 $\beta$는 0 근처에 모여있었겠죠.</p>
<p>비슷하게 $\alpha$의 사후확률분포는 0과 멀리 떨어져있는 음수값이 나옵니다. 이 또한 $\alpha$값이 0보다 확실히 작은 숫자라는 믿음을 추론하게 합니다.</p>
<p>데이터가 많이 퍼져있기 때문에, 실제 모수가 무었인지는 확신하지 못합니다.(샘플사이즈가 일단 작고, 손상이 있을 때의 기온과 없을 때의 기온이 많이 겹쳐서 그렇다고 예상해봅니다)</p>
<p>다음으로 특정한 온도에서의 예측 확률을 구해봅시다. 즉 특정 $t$에서 나올 수 있는 확률 $p(t_i)$의 평균을 구해봅시다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">alpha_samples_1d_ <span style="color:#f92672">=</span> posterior_alpha_[:, None]  <span style="color:#75715e"># best to make them 1d</span>
beta_samples_1d_ <span style="color:#f92672">=</span> posterior_beta_[:, None]

beta_mean <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(beta_samples_1d_<span style="color:#f92672">.</span>T[<span style="color:#ae81ff">0</span>])
alpha_mean <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(alpha_samples_1d_<span style="color:#f92672">.</span>T[<span style="color:#ae81ff">0</span>])
[ beta_mean_, alpha_mean_ ] <span style="color:#f92672">=</span> evaluate([ beta_mean, alpha_mean ])


<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;beta mean:&#34;</span>, beta_mean_)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;alpha mean:&#34;</span>, alpha_mean_)
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">logistic</span>(x, beta, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    alpha와 beta에 대한 로지스틱 함수
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">    Args:
</span><span style="color:#e6db74">      x: 독립 변수
</span><span style="color:#e6db74">      beta: beta 항
</span><span style="color:#e6db74">      alpha: alpha 항
</span><span style="color:#e6db74">    Returns: 
</span><span style="color:#e6db74">      로지스틱 함수
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">+</span> tf<span style="color:#f92672">.</span>exp((beta <span style="color:#f92672">*</span> x) <span style="color:#f92672">+</span> alpha))

t_ <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(temperature_<span style="color:#f92672">.</span>min() <span style="color:#f92672">-</span> <span style="color:#ae81ff">5</span>, temperature_<span style="color:#f92672">.</span>max() <span style="color:#f92672">+</span> <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">2500</span>)[:, None]
p_t <span style="color:#f92672">=</span> logistic(t_<span style="color:#f92672">.</span>T, beta_samples_1d_, alpha_samples_1d_)
mean_prob_t <span style="color:#f92672">=</span> logistic(t_<span style="color:#f92672">.</span>T, beta_mean_, alpha_mean_)
[ 
    p_t_, mean_prob_t_
] <span style="color:#f92672">=</span> evaluate([ 
    p_t, mean_prob_t
])
</code></pre></div><pre><code>beta mean: 0.25206435
alpha mean: -16.329533
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt<span style="color:#f92672">.</span>figure(figsize(<span style="color:#ae81ff">12.5</span>, <span style="color:#ae81ff">4</span>))

plt<span style="color:#f92672">.</span>plot(t_, mean_prob_t_<span style="color:#f92672">.</span>T, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;손상의 평균 사후 확률&#34;</span>)
plt<span style="color:#f92672">.</span>plot(t_, p_t_<span style="color:#f92672">.</span>T[:, <span style="color:#ae81ff">0</span>], ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;--&#34;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;사후 분포의 실현값&#34;</span>)
plt<span style="color:#f92672">.</span>plot(t_, p_t_<span style="color:#f92672">.</span>T[:, <span style="color:#f92672">-</span><span style="color:#ae81ff">8</span>], ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;--&#34;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;사후 분포의 실현값&#34;</span>)
plt<span style="color:#f92672">.</span>scatter(temperature_, D_, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;k&#34;</span>, s<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;두 가지 실현값과 손상 확률의 사후 기댓값&#34;</span>)
plt<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lower left&#34;</span>)
plt<span style="color:#f92672">.</span>ylim(<span style="color:#f92672">-</span><span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">1.1</span>)
plt<span style="color:#f92672">.</span>xlim(t_<span style="color:#f92672">.</span>min(), t_<span style="color:#f92672">.</span>max())
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;확률&#34;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;기온&#34;</span>);
</code></pre></div><p><img src="https://user-images.githubusercontent.com/57588650/92322196-c6290c80-f06a-11ea-9fef-c89495a62b4b.png" alt="output_27_0"></p>
<p>위에서 우리는 구현 가능한 실제 내부 시스템 두 가지를 그래프로 그렸습니다. 두 개는 제비뽑기와 같이 같은 확률을 가지고 있습니다. 파란선은 모든 가능한 20000개의 점선을 평균낸 값이죠.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> scipy.stats.mstats <span style="color:#f92672">import</span> mquantiles

<span style="color:#75715e"># 95% 신뢰구간 구하기</span>
qs <span style="color:#f92672">=</span> mquantiles(p_t_, [<span style="color:#ae81ff">0.025</span>, <span style="color:#ae81ff">0.975</span>], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
plt<span style="color:#f92672">.</span>fill_between(t_[:, <span style="color:#ae81ff">0</span>], <span style="color:#f92672">*</span>qs, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7</span>,
                 color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;#7A68A6&#34;</span>)

plt<span style="color:#f92672">.</span>plot(t_[:, <span style="color:#ae81ff">0</span>], qs[<span style="color:#ae81ff">0</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;95% CI&#34;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;#7A68A6&#34;</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7</span>)

plt<span style="color:#f92672">.</span>plot(t_[:, <span style="color:#ae81ff">0</span>], mean_prob_t_[<span style="color:#ae81ff">0</span>,:], lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;--&#34;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;k&#34;</span>,
         label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;손상의 평균 사후 확률&#34;</span>)

plt<span style="color:#f92672">.</span>xlim(t_<span style="color:#f92672">.</span>min(), t_<span style="color:#f92672">.</span>max())
plt<span style="color:#f92672">.</span>ylim(<span style="color:#f92672">-</span><span style="color:#ae81ff">0.02</span>, <span style="color:#ae81ff">1.02</span>)
plt<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lower left&#34;</span>)
plt<span style="color:#f92672">.</span>scatter(temperature_, D_, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;k&#34;</span>, s<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;temp, $t$&#34;</span>)

plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;추정 확률&#34;</span>)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;특정 온도 $t$에서의 사후 확률 추정&#34;</span>);
</code></pre></div><p><img src="https://user-images.githubusercontent.com/57588650/92322203-cd501a80-f06a-11ea-8b4b-30f7eb8dacbb.png" alt="output_29_0"></p>
<p>보라색으로 칠해진 95% 신뢰구간(Credible interval. CI)는 각각의 온도에 대해 95%의 분포를 포함하는 구간을 나타냅니다. 예를 들면 65도F에서 손상율이 25%~85% 사이에 있다고 95% 확신할 수 있습니다.</p>
<p>더 일반적으로 말하자면, 60도 근처에서 0과 1 사이에 빠르게 점점 넓게 퍼지지만, 70도가 넘어가면 다시 좁아진다는 것을 알 수 있습니다. 이제 우리가 다음에 어떤 절차를 거쳐야 할지를 알려줍니다. 범위가 넓은 60에서 65도 사이의 테스트 데이터를 더 확보해 더 나은 확률을 추정해야죠. 비슷하게, 당신의 추정을 과학자들에게 보고할 때 당신은 예측 확률이 몇 퍼센트라고 간단하게 말하는 것에 주의해야합니다. 그것은 우리의 사후 분포가 얼마나 넓게 퍼져있는지를 반영하지 않기 때문이죠.</p>
<h2 id="그렇다면-챌린저-사고가-일어난-날은-어떨까"><strong>그렇다면 챌린저 사고가 일어난 날은 어떨까?</strong></h2>
<p>챌린저 사고가 일어난 날, 외부 기온은 화씨 31도였습니다. 그렇다면 이 온도에서 손상 발생의 사후 분포는 어떨까요? 그 분포는 밑에서 그려보겠습니다. 그래프를 보면 챌린저호가 손상된 O-ring의 피해자가 될 것이란걸 확신할 수 있죠.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt<span style="color:#f92672">.</span>figure(figsize(<span style="color:#ae81ff">12.5</span>, <span style="color:#ae81ff">3</span>))

<span style="color:#75715e"># 로지스틱 함수에 온도 31, 사후 베타 분포, 사후 알파 분포를 넣습니다</span>
prob_31 <span style="color:#f92672">=</span> logistic(<span style="color:#ae81ff">31</span>, posterior_beta_, posterior_alpha_)

<span style="color:#75715e"># 함수를 실행합시다</span>
[ prob_31_ ] <span style="color:#f92672">=</span> evaluate([ prob_31 ])

<span style="color:#75715e"># 히스토그램을 그립시다.</span>
plt<span style="color:#f92672">.</span>xlim(<span style="color:#ae81ff">0.95</span>, <span style="color:#ae81ff">1</span>)
plt<span style="color:#f92672">.</span>hist(prob_31_, bins<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, density<span style="color:#f92672">=</span>True, histtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;stepfilled&#39;</span>)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;$t = 31$일 때의 손상 확률 사후 분포&#34;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;O-ring의 손상이 발생할 확률&#34;</span>);
</code></pre></div><p><img src="https://user-images.githubusercontent.com/57588650/92322207-d4772880-f06a-11ea-9a2d-91dd9cc82eb7.png" alt="output_33_0"></p>
<h3 id="우리의-모델이-적절한가요"><strong>우리의 모델이 적절한가요?</strong></h3>
<p>시크한 독자들은 &ldquo;너 맘대로 $p(t)$를 로지스틱 함수라고 하고 맘대로 사전 믿음을 정했잖아. 그런데 다른 함수를 쓰면 다른 결과가 나올 수 있어. 우리가 좋은 모델을 고른거라고 어떻게 알 수 있지?&ldquo;라고 물어볼 수 있습니다. 이 질문은 명백한 사실입니다. 극단적인 상황을 고려하기 위해, 모든 $t$에서 항상결함이 발생하는 $p(t) = 1$이라고 가정합시다. 자 이제 다시 1월 28일에서의 사고 발생을 예측해보겠습니다. 그러나 이것은 잘못된 모델 선택임이 확실합니다. 반대로 만약 $p(t)$를 로지스틱 함수로 정하고 사전 확률을 0 근처에 모여있다고 가정한다면 아주 다른 사후 분포가 나올 것입니다. 어떻게 우리의 모델이 데이터를 잘 표현한다는 것을 알 수 있을까요? 이 질문은 우리가 모델의 &lsquo;<strong>goodness of fit</strong>&lsquo;을 측정하게 합니다.</p>
<p>어떻게 우리의 모델이 안좋은지 좋은지 테스트 할 수 있지? 라는 질문을 할 수 있습니다. 이를 위한 아이디어는 바로 관측 데이터와 시뮬레이션을 통해 만든 인공 데이터를 비교해보는거죠. 논리는 이렇습니다. 만약 시뮬레이션된 데이터셋이 통계적으로 관측된 데이터와 비슷하지 않다면, 우리의 모델은 관측 데이터를 정확하게 나타내지 못할 확률이 높습니다.</p>
<p>Chapter 1에서 우리는 문자 메시지 예제의 인공 데이터셋을 시뮬레이션 해보았습니다. 이것을 하기 위해 우리는 사전 분포에서 값들을 뽑았습니다. 우리는 얼마나 데이터셋이 다양하고 관측치를 잘 따라하지 못한다는 것을 알 수 있었습니다. 이번 예제에서 우리는 아주 그럴듯한 데이터셋을 뽑기 위해 사후 분포에서 시뮬레이션 해보도록 하겠습니다. 운이 좋게도 우리의 베이지안 기초 공사는 이것을 매우 쉽게 할 수 있게 합니다. 우리는 단지 선택한 분포에서 샘플들을 모으고, 샘플의 갯수를 정하고, 샘플의 모양을 정하기만 하면 됩니다(우리의 원래 데이터에 21개의 관측치가 았으므로 우리도 21개를 뽑겠습니다.). 그리고 1로 관측한 값과 0으로 관측한 값의 비율을 알기 위한 확률을 정해야죠.</p>
<p>그래서 우리는 다음과 같은 것을 만들 수 있습니다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">simulated_data <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Bernoulli(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;simulation_data&#34;</span>, probs<span style="color:#f92672">=</span>p)<span style="color:#f92672">.</span>sample(sample_shape<span style="color:#f92672">=</span>N)
</code></pre></div><p>10000 번 시뮬레이션 해봅시다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">alpha <span style="color:#f92672">=</span> alpha_mean_ <span style="color:#75715e"># 위의 HMC 모델에서 세 개의 값을 뽑아내도록 하겠습니다.</span>
beta <span style="color:#f92672">=</span> beta_mean_
p_deterministic <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Deterministic(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;p&#34;</span>, loc<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span><span style="color:#f92672">/</span>(<span style="color:#ae81ff">1.</span> <span style="color:#f92672">+</span> tf<span style="color:#f92672">.</span>exp(beta <span style="color:#f92672">*</span> temperature_ <span style="color:#f92672">+</span> alpha)))<span style="color:#f92672">.</span>sample()<span style="color:#75715e">#seed=6.45)</span>

simulated_data <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Bernoulli(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;bernoulli_sim&#34;</span>, 
                               probs<span style="color:#f92672">=</span>p_deterministic_)<span style="color:#f92672">.</span>sample(sample_shape<span style="color:#f92672">=</span><span style="color:#ae81ff">10000</span>)
[ 
    bernoulli_sim_samples_,
    p_deterministic_
] <span style="color:#f92672">=</span>evaluate([
    simulated_data,
    p_deterministic
])
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">simulations_ <span style="color:#f92672">=</span> bernoulli_sim_samples_
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Number of simulations:             &#34;</span>, simulations_<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>])
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Number data points per simulation: &#34;</span>, simulations_<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>])

plt<span style="color:#f92672">.</span>figure(figsize(<span style="color:#ae81ff">12.5</span>, <span style="color:#ae81ff">12</span>))
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;Simulated dataset using posterior parameters&#34;</span>)
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">4</span>):
    ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>, i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)
    plt<span style="color:#f92672">.</span>scatter(temperature_, simulations_[<span style="color:#ae81ff">1000</span><span style="color:#f92672">*</span>i, :], color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;k&#34;</span>,
                s<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.6</span>)
    
</code></pre></div><pre><code>Number of simulations:              10000
Number data points per simulation:  23
</code></pre>
<p><img src="https://user-images.githubusercontent.com/57588650/92322211-dd67fa00-f06a-11ea-84dc-9d9dafdb8b8c.png" alt="output_38_1"></p>
<p>위의 그래프들은 모두 다릅니다.</p>
<p>자 이제 우리는 우리의 모델이 얼마나 좋은지 알고싶습니다. &ldquo;좋은&quot;이란 말은 당연히 주관적인 용어입니다. 그래서 결과는 다른 모델에 대해 상대적입니다.</p>
<p>우리는 이것 또한 덜 객관적으로 보이는 방법인 그래프를 그려서 얼마나 다른지를 보는 방법으로도 할 수 있습니다. 대안은 바로 베이지안 p-value를 사용하는 것입니다. 이것도 역시 좋은 것과 안좋은것을 나누는 경계선을 임의로 정하기 때문에 여전히 주관적이긴 합니다. Gelman은 그래프를 그려서 하는 테스트가 p-value 테스트보다 더 이해하기 쉽다는 점을 강조합니다[3]. 저 또한 그렇게 생각하죠.</p>
<p>다음의 그래프 테스트는 로지스틱 회귀 분석을 위한 새로운 데이터 시각화 방법입니다. 이 그래프들은 *분리 도표(seperation plot)*라고 불립니다. 우리가 비교하고 싶은 각각의 모델들을 각각 나뉘어진 그래프로 그림으로서 서로 비교합니다. 분리 도표의 기술적인 디테일에 대해서는 <a href="http://mdwardlab.com/sites/default/files/GreenhillWardSacks.pdf">논문</a>의 링크를 남기도록 하겠습니다. 여기서는 이 내용을 요약해보죠.</p>
<p>각각의 모델에 우리는 특정 온도에서 사후 시뮬레이션이 1의 값을 반환하는 비율을 계산합니다. 예를 들어 $P(\text{Defect} = 1 | t, \alpha, \beta)$의 평균을 구합니다. 이것은 우리 데이터셋의 각각의 지점에서의 사후 확률을 반환합니다. 예를 들어 위에서 만든 모델에 대해선 다음과 같이 구할 수 있습니다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">posterior_probability_ <span style="color:#f92672">=</span> simulations_<span style="color:#f92672">.</span>mean(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;posterior prob of defect | realized defect &#34;</span>)
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(D_)):
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;</span><span style="color:#e6db74">%.2f</span><span style="color:#e6db74">                     |   </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (posterior_probability_[i], D_[i]))
</code></pre></div><pre><code>posterior prob of defect | realized defect 
0.43                     |   0
0.21                     |   1
0.25                     |   0
0.31                     |   0
0.37                     |   0
0.14                     |   0
0.11                     |   0
0.21                     |   0
0.87                     |   1
0.61                     |   1
0.21                     |   1
0.04                     |   0
0.37                     |   0
0.95                     |   1
0.37                     |   0
0.07                     |   0
0.21                     |   0
0.02                     |   0
0.06                     |   0
0.03                     |   0
0.07                     |   1
0.06                     |   0
0.84                     |   1
</code></pre>
<p>다음으로 사후 확률에 대해 각각의 열을 오름차순으로 정렬합니다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">ix_ <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argsort(posterior_probability_)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;probb | defect &#34;</span>)
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(D_)):
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;</span><span style="color:#e6db74">%.2f</span><span style="color:#e6db74">  |   </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (posterior_probability_[ix_[i]], D_[ix_[i]]))
</code></pre></div><pre><code>probb | defect 
0.02  |   0
0.03  |   0
0.04  |   0
0.06  |   0
0.06  |   0
0.07  |   1
0.07  |   0
0.11  |   0
0.14  |   0
0.21  |   1
0.21  |   0
0.21  |   0
0.21  |   1
0.25  |   0
0.31  |   0
0.37  |   0
0.37  |   0
0.37  |   0
0.43  |   0
0.61  |   1
0.84  |   1
0.87  |   1
0.95  |   1
</code></pre>
<p>이제 우리는 위의 데이터를 그래프로 더 잘 나타낼 수 있습니다. 자 이제 <code>separation_plot</code> 함수를 만들어보겠습니다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">separation_plot</span>( p, y, <span style="color:#f92672">**</span>kwargs ):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    이 함수는 로지스틱 그리고 프로빗(probit) 분류의 분리도표를 출력합니다.
</span><span style="color:#e6db74">    http://mdwardlab.com/sites/default/files/GreenhillWardSacks.pdf 이 사이트를 보세요
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    p: M개의 모델을 나타내는 확률/비율이 들어있는 nxM 행렬
</span><span style="color:#e6db74">    y: 0부터 1까지의 응답 변수
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>    
    <span style="color:#66d9ef">assert</span> p<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> y<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#e6db74">&#34;p.shape[0] != y.shape[0]&#34;</span>
    n <span style="color:#f92672">=</span> p<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]

    <span style="color:#66d9ef">try</span>:
        M <span style="color:#f92672">=</span> p<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
    <span style="color:#66d9ef">except</span>:
        p <span style="color:#f92672">=</span> p<span style="color:#f92672">.</span>reshape( n, <span style="color:#ae81ff">1</span> )
        M <span style="color:#f92672">=</span> p<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]

    colors_bmh <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array( [<span style="color:#e6db74">&#34;#eeeeee&#34;</span>, <span style="color:#e6db74">&#34;#348ABD&#34;</span>] )


    fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>figure( )
    
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(M):
        ax <span style="color:#f92672">=</span> fig<span style="color:#f92672">.</span>add_subplot(M, <span style="color:#ae81ff">1</span>, i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)
        ix <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argsort( p[:,i] )
        <span style="color:#75715e">#plot the different bars</span>
        bars <span style="color:#f92672">=</span> ax<span style="color:#f92672">.</span>bar( np<span style="color:#f92672">.</span>arange(n), np<span style="color:#f92672">.</span>ones(n), width<span style="color:#f92672">=</span><span style="color:#ae81ff">1.</span>,
                color <span style="color:#f92672">=</span> colors_bmh[ y[ix]<span style="color:#f92672">.</span>astype(int) ], 
                edgecolor <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;none&#39;</span>)
        ax<span style="color:#f92672">.</span>plot( np<span style="color:#f92672">.</span>arange(n<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>), np<span style="color:#f92672">.</span>append(p[ix,i], p[ix,i][<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]), <span style="color:#e6db74">&#34;k&#34;</span>,
                 linewidth <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.</span>,drawstyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;steps-post&#34;</span> )
        <span style="color:#75715e">#create expected value bar.</span>
        ax<span style="color:#f92672">.</span>vlines( [(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>p[ix,i])<span style="color:#f92672">.</span>sum()], [<span style="color:#ae81ff">0</span>], [<span style="color:#ae81ff">1</span>] )
        plt<span style="color:#f92672">.</span>xlim( <span style="color:#ae81ff">0</span>, n)
        
    plt<span style="color:#f92672">.</span>tight_layout()
    
    <span style="color:#66d9ef">return</span>

plt<span style="color:#f92672">.</span>figure(figsize(<span style="color:#ae81ff">11.</span>, <span style="color:#ae81ff">3</span>))
separation_plot(posterior_probability_, D_)
</code></pre></div><pre><code>&lt;Figure size 792x216 with 0 Axes&gt;
</code></pre>
<p><img src="https://user-images.githubusercontent.com/57588650/92322217-e8228f00-f06a-11ea-9328-c47dab7f0e2a.png" alt="output_44_1"></p>
<p>뱀같은 선은 정렬된 확률이고 파란 막대는 손상을 의미합니다. 그리고 빈 공간은 손상이 없음을 의미하죠. 확률이 올라갈 수록 더 많은 손상이 나타난다는 것을 볼 수 있습니다. 오른쪽에서, 그래프는 사후 확률이 더 커질 수록 실제로도 손상이 많이 발생했다는 것을 알 수 있습니다. 이것은 좋은 결과입니다. 이상적으로 모든 파란 막대가 그래프의 맨 오른쪽에 있어야 합니다. 이상적인 것과 우리의 결과 사이의 편차는 예측이 찾아내지 못한 것을 반영합니다.</p>
<p>까만 수직선은 우리가 이 모델에서 관찰할 수 있는 손상의 수의 기댓값을 나타냅니다. 이것은 사용자들이 실제 데이터에서의 사건의 수와 예측된 총 사건의 수를 비교할 수 있게 해줍니다.</p>
<p>다른 모델들을 분리 도표를 통해 이것을 비교하는게 더 많은 정보를 줍니다. 위에서 만든 모델과 다른 세 가지 모델을 비교해봅시다.</p>
<ol>
<li>
<p>실제로 손상이 발생하면 사후 확률로 1인 완벽한 모델입니다.</p>
</li>
<li>
<p>완전히 무작위의 모델입니다. 기온과 상관없는 무작위 확률이죠.</p>
</li>
<li>
<p>상수 모델입니다. $P(D = 1| t) = c$와 같죠. $c$를 정하는 최고의 방법은 관찰된 손상의 빈도인 7/23을 사용하는겁니다.</p>
</li>
</ol>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt<span style="color:#f92672">.</span>figure(figsize(<span style="color:#ae81ff">11.</span>, <span style="color:#ae81ff">2</span>))

<span style="color:#75715e"># Our temperature-dependent model</span>
separation_plot(posterior_probability_, D_)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;기온에 따른 모델&#34;</span>)

<span style="color:#75715e"># Perfect model</span>
<span style="color:#75715e"># i.e. the probability of defect is equal to if a defect occurred or not.</span>
p_ <span style="color:#f92672">=</span> D_
separation_plot(p_, D_)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;완벽한 모델&#34;</span>)

<span style="color:#75715e"># random predictions</span>
p_ <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">23</span>)
separation_plot(p_, D_)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;무작위 모델&#34;</span>)

<span style="color:#75715e"># constant model</span>
constant_prob_ <span style="color:#f92672">=</span> <span style="color:#ae81ff">7.</span><span style="color:#f92672">/</span><span style="color:#ae81ff">23</span> <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">23</span>)
separation_plot(constant_prob_, D_)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;상수 예측 모델&#34;</span>);
</code></pre></div><pre><code>&lt;Figure size 792x144 with 0 Axes&gt;
</code></pre>
<p><img src="https://user-images.githubusercontent.com/57588650/92322220-efe23380-f06a-11ea-93fe-7cf4228e189c.png" alt="output_46_1"></p>
<p><img src="https://user-images.githubusercontent.com/57588650/92322226-f8d30500-f06a-11ea-9260-07838204adbc.png" alt="output_46_2"></p>
<p><img src="https://user-images.githubusercontent.com/57588650/92322228-fa043200-f06a-11ea-8ee8-b979cca09d01.png" alt="output_46_3"></p>
<p><img src="https://user-images.githubusercontent.com/57588650/92322230-fb355f00-f06a-11ea-9c79-76b7b97f0653.png" alt="output_46_4"></p>
<p>무작위 모델에서, 확률이 오를 수록 오른쪽에 파란 막대가 모이는 경향이 없다는 것을 알 수 있습니다.</p>
<p>완벽한 모델에서 확률 선은 잘 보이지 않습니다. 그래프의 맨 밑과 맨 위에 고정되어있기 때문이죠. 당연히 완벽한 모델은 단지 보여주기 위한 것이므로 아무런 과학적인 추론도 할 수 없습니다.</p>
<h2 id="예제">예제</h2>
<ol>
<li>
<p>치팅 예제에서 우리의 관찰값이 극단적인 값을 가진다고 가정합시다. 만일 치팅을 했다는 대답을 25번, 10번, 50번 받았다면 어떻게 될까요?</p>
</li>
<li>
<p>$\alpha$의 표본과 $\beta$의 표본을 뽑아서 그래프를 그려보고 대조해봅시다. 왜 결과 그래프가 이렇게 나올까요?</p>
</li>
</ol>
<pre><code># 그래프 그리는 코드
plt.figure(figsize(12.5, 4))

plt.scatter(alpha_samples_, beta_samples_, alpha=0.1)
plt.title(&quot;Why does the plot look like this?&quot;)
plt.xlabel(r&quot;$\alpha$&quot;)
plt.ylabel(r&quot;$\beta$&quot;);
</code></pre><h2 id="references"><strong>References</strong></h2>
<p>[1] Dalal, Fowlkes and Hoadley (1989),JASA, 84, 945-957.</p>
<p>[2] Cronin, Beau. &ldquo;Why Probabilistic Programming Matters.&rdquo; 24 Mar 2013. Google, Online Posting to Google . Web. 24 Mar. 2013. <a href="https://plus.google.com/u/0/+BeauCronin/posts/KpeRdJKR6Z1">https://plus.google.com/u/0/+BeauCronin/posts/KpeRdJKR6Z1</a>.</p>
<p>[3] Gelman, Andrew. &ldquo;Philosophy and the practice of Bayesian statistics.&rdquo; British Journal of Mathematical and Statistical Psychology. (2012): n. page. Web. 2 Apr. 2013.</p>
<p>[4] Greenhill, Brian, Michael D. Ward, and Audrey Sacks. &ldquo;The Separation Plot: A New Visual Method for Evaluating the Fit of Binary Models.&rdquo; American Journal of Political Science. 55.No.4 (2011): n. page. Web. 2 Apr. 2013.</p>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/bayesian/" rel="tag">Bayesian</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/tensorflow/" rel="tag">TensorFlow</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/python/" rel="tag">Python</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div class="authorbox clearfix">
	<div class="authorbox__header">
		<span class="authorbox__name"></span>
	</div>
</div>



			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2021 Tai Hwan Oh.
			<span class="footer__copyright-credits"></span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>